{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import IPython\n",
    "IPython.get_ipython().run_line_magic('cd', '..')  # Go up one directory\n",
    "\n",
    "from dictionary_learning.buffer import AllActivationBuffer\n",
    "from dictionary_learning.trainers.top_k import TrainerSCAE, AutoEncoderTopK\n",
    "from dictionary_learning.training import trainSCAE\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch as t\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "#\n",
    "device = \"cuda:0\" if t.cuda.is_available() else \"cpu\"\n",
    "model = LanguageModel(\"gpt2\", device_map=device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "dataset = load_dataset(\n",
    "    'Skylion007/openwebtext', \n",
    "    split='train', \n",
    "    streaming=True,\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "\n",
    "class CustomData():\n",
    "    def __init__(self, dataset):\n",
    "        self.data = iter(dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.data)['text']\n",
    "\n",
    "data = CustomData(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 20\n",
    "expansion = 16\n",
    "k= 128 # TODO\n",
    "\n",
    "num_features = model.config.n_embd * expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "n_layer = model.config.n_layer\n",
    "\n",
    "submodules = {}\n",
    "for layer in range(n_layer):\n",
    "    submodules[f\"mlp_{layer}\"] = (model.transformer.h[layer].mlp, \"in_and_out\")\n",
    "    submodules[f\"attn_{layer}\"] = (model.transformer.h[layer].attn, \"out\")\n",
    "\n",
    "\n",
    "buffer = AllActivationBuffer(\n",
    "    data=data,\n",
    "    model=model,\n",
    "    submodules=submodules,\n",
    "    d_submodule=model.config.n_embd, # output dimension of the model component\n",
    "    n_ctxs=128,  # you can set this higher or lower depending on your available memory\n",
    "    device=\"cuda\",\n",
    "    out_batch_size = 512,\n",
    "    refresh_batch_size = 256,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = {f\"mlp_{layer}\": t.randint(0, num_features, (num_features, C))\n",
    "                        for layer in range(n_layer)}\n",
    "\n",
    "# Get submodule names from the submodules dictionary\n",
    "submodule_names = list(submodules.keys())\n",
    "\n",
    "# Configure the multi-trainer\n",
    "trainer_cfg = {\n",
    "    \"trainer\": TrainerSCAE,\n",
    "    \"activation_dims\": {name: model.config.n_embd for name in submodule_names},\n",
    "    \"dict_sizes\": {name: model.config.n_embd * expansion for name in submodule_names},\n",
    "    \"ks\": {name: k for name in submodule_names},\n",
    "    \"device\": buffer.device,\n",
    "    \"submodules\": submodules,\n",
    "    \"important_features\": important_features,\n",
    "}\n",
    "\n",
    "trainer_class = trainer_cfg.pop(\"trainer\")\n",
    "trainer = trainer_class(**trainer_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = next(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(256201., device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.loss(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 134148.3438:   0%|          | 35/30000 [04:15<60:48:21,  7.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer_cfg \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainer\u001b[39m\u001b[38;5;124m\"\u001b[39m: TrainerSCAE,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation_dims\u001b[39m\u001b[38;5;124m\"\u001b[39m: {name: model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_embd \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m submodule_names},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimportant_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: important_features,\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Run the training\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtrainSCAE\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msae_checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to False if you don't want to use wandb\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#aa9a791c5e40fa7ab2f08d555ff72352c1cecaa2\u001b[39;00m\n",
      "File \u001b[0;32m~/dictionary_learning/training.py:268\u001b[0m, in \u001b[0;36mtrainSCAE\u001b[0;34m(buffer, trainer_cfg, steps, save_steps, save_dir, log_steps, use_wandb)\u001b[0m\n\u001b[1;32m    262\u001b[0m             t\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m    263\u001b[0m                 ae\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m    264\u001b[0m                 os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mae_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    265\u001b[0m             )\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_acts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Save final models\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/dictionary_learning/trainers/top_k.py:686\u001b[0m, in \u001b[0;36mTrainerSCAE.update\u001b[0;34m(self, step, input_acts, target_acts)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;66;03m# Compute loss and backward pass\u001b[39;00m\n\u001b[1;32m    685\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(input_acts, target_acts, step\u001b[38;5;241m=\u001b[39mstep)\n\u001b[0;32m--> 686\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;66;03m# Clip gradients and remove parallel components\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ae \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maes\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_cfg = {\n",
    "    \"trainer\": TrainerSCAE,\n",
    "    \"activation_dims\": {name: model.config.n_embd for name in submodule_names},\n",
    "    \"dict_sizes\": {name: model.config.n_embd * expansion for name in submodule_names},\n",
    "    \"ks\": {name: k for name in submodule_names},\n",
    "    \"device\": buffer.device,\n",
    "    \"submodules\": submodules,\n",
    "    \"important_features\": important_features,\n",
    "}\n",
    "\n",
    "# Run the training\n",
    "trainer = trainSCAE(\n",
    "    buffer=buffer,\n",
    "    trainer_cfg=trainer_cfg,\n",
    "    steps=30000,\n",
    "    save_steps=1000,\n",
    "    save_dir=\"sae_checkpoints\",\n",
    "    log_steps=100,\n",
    "    use_wandb=False  # Set to False if you don't want to use wandb\n",
    ")\n",
    "\n",
    "#aa9a791c5e40fa7ab2f08d555ff72352c1cecaa2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vanilla_encode_mlp_0': [37.3493766784668],\n",
       " 'vanilla_decode_mlp_0': [0.8058879971504211],\n",
       " 'vanilla_forward_mlp_0': [49.53209686279297],\n",
       " 'vanilla_encode_attn_0': [1.213088035583496],\n",
       " 'vanilla_decode_attn_0': [0.5580800175666809],\n",
       " 'vanilla_forward_attn_0': [2.357599973678589],\n",
       " 'vanilla_encode_mlp_1': [1.494271993637085],\n",
       " 'vanilla_decode_mlp_1': [0.5468159914016724],\n",
       " 'vanilla_forward_mlp_1': [2.524224042892456],\n",
       " 'vanilla_encode_attn_1': [1.1706880331039429],\n",
       " 'vanilla_decode_attn_1': [0.5386239886283875],\n",
       " 'vanilla_forward_attn_1': [2.1621758937835693],\n",
       " 'vanilla_encode_mlp_2': [1.1542079448699951],\n",
       " 'vanilla_decode_mlp_2': [0.5468159914016724],\n",
       " 'vanilla_forward_mlp_2': [2.1412160396575928],\n",
       " 'vanilla_encode_attn_2': [1.157312035560608],\n",
       " 'vanilla_decode_attn_2': [0.536575973033905],\n",
       " 'vanilla_forward_attn_2': [2.1166720390319824],\n",
       " 'vanilla_encode_mlp_3': [1.3451839685440063],\n",
       " 'vanilla_decode_mlp_3': [0.5355520248413086],\n",
       " 'vanilla_forward_mlp_3': [2.3292479515075684],\n",
       " 'vanilla_encode_attn_3': [1.1627520322799683],\n",
       " 'vanilla_decode_attn_3': [0.5283839702606201],\n",
       " 'vanilla_forward_attn_3': [2.0956480503082275],\n",
       " 'vanilla_encode_mlp_4': [1.1478400230407715],\n",
       " 'vanilla_decode_mlp_4': [0.5294079780578613],\n",
       " 'vanilla_forward_mlp_4': [2.0736639499664307],\n",
       " 'vanilla_encode_attn_4': [1.1379519701004028],\n",
       " 'vanilla_decode_attn_4': [0.5263360142707825],\n",
       " 'vanilla_forward_attn_4': [2.046175956726074],\n",
       " 'vanilla_encode_mlp_5': [1.3344000577926636],\n",
       " 'vanilla_decode_mlp_5': [0.5355520248413086],\n",
       " 'vanilla_forward_mlp_5': [2.268512010574341],\n",
       " 'vanilla_encode_attn_5': [1.1402239799499512],\n",
       " 'vanilla_decode_attn_5': [0.5304319858551025],\n",
       " 'vanilla_forward_attn_5': [2.0606400966644287],\n",
       " 'vanilla_encode_mlp_6': [1.1416319608688354],\n",
       " 'vanilla_decode_mlp_6': [0.5294079780578613],\n",
       " 'vanilla_forward_mlp_6': [2.0504000186920166],\n",
       " 'vanilla_encode_attn_6': [1.1486079692840576],\n",
       " 'vanilla_decode_attn_6': [0.5386239886283875],\n",
       " 'vanilla_forward_attn_6': [2.3185598850250244],\n",
       " 'vanilla_encode_mlp_7': [1.3079359531402588],\n",
       " 'vanilla_decode_mlp_7': [0.5314559936523438],\n",
       " 'vanilla_forward_mlp_7': [2.214143991470337],\n",
       " 'vanilla_encode_attn_7': [1.1384320259094238],\n",
       " 'vanilla_decode_attn_7': [0.5263360142707825],\n",
       " 'vanilla_forward_attn_7': [2.0299839973449707],\n",
       " 'vanilla_encode_mlp_8': [1.1354559659957886],\n",
       " 'vanilla_decode_mlp_8': [0.5314559936523438],\n",
       " 'vanilla_forward_mlp_8': [2.036223888397217],\n",
       " 'vanilla_encode_attn_8': [1.3014719486236572],\n",
       " 'vanilla_decode_attn_8': [0.5294079780578613],\n",
       " 'vanilla_forward_attn_8': [2.202847957611084],\n",
       " 'vanilla_encode_mlp_9': [1.1504640579223633],\n",
       " 'vanilla_decode_mlp_9': [0.5294079780578613],\n",
       " 'vanilla_forward_mlp_9': [2.0648000240325928],\n",
       " 'vanilla_encode_attn_9': [1.1428159475326538],\n",
       " 'vanilla_decode_attn_9': [0.5273600220680237],\n",
       " 'vanilla_forward_attn_9': [2.0445120334625244],\n",
       " 'vanilla_encode_mlp_10': [1.142016053199768],\n",
       " 'vanilla_decode_mlp_10': [0.5263360142707825],\n",
       " 'vanilla_forward_mlp_10': [2.038815975189209],\n",
       " 'vanilla_encode_attn_10': [1.2978559732437134],\n",
       " 'vanilla_decode_attn_10': [0.5263040065765381],\n",
       " 'vanilla_forward_attn_10': [2.210207939147949],\n",
       " 'vanilla_encode_mlp_11': [1.136639952659607],\n",
       " 'vanilla_decode_mlp_11': [0.5273600220680237],\n",
       " 'vanilla_forward_mlp_11': [2.045759916305542],\n",
       " 'vanilla_encode_attn_11': [1.1390399932861328],\n",
       " 'vanilla_decode_attn_11': [0.522271990776062],\n",
       " 'vanilla_forward_attn_11': [2.033087968826294],\n",
       " 'vanilla_forward': [100.2481918334961],\n",
       " 'get_approx_inputs_mlp_0': [0.053568001836538315],\n",
       " 'get_approx_inputs_attn_0': [0.009216000325977802],\n",
       " 'get_approx_inputs_mlp_1': [10.262528419494629],\n",
       " 'get_approx_inputs_attn_1': [0.010239999741315842],\n",
       " 'get_approx_inputs_mlp_2': [20.310016632080078],\n",
       " 'get_approx_inputs_attn_2': [0.010239999741315842],\n",
       " 'get_approx_inputs_mlp_3': [30.325759887695312],\n",
       " 'get_approx_inputs_attn_3': [0.009216000325977802],\n",
       " 'get_approx_inputs_mlp_4': [40.3138542175293],\n",
       " 'get_approx_inputs_attn_4': [0.011264000087976456],\n",
       " 'get_approx_inputs_mlp_5': [50.332672119140625],\n",
       " 'get_approx_inputs_attn_5': [0.009216000325977802],\n",
       " 'get_approx_inputs_mlp_6': [60.47539138793945],\n",
       " 'get_approx_inputs_attn_6': [0.010239999741315842],\n",
       " 'get_approx_inputs_mlp_7': [70.45222473144531],\n",
       " 'get_approx_inputs_attn_7': [0.009216000325977802],\n",
       " 'get_approx_inputs_mlp_8': [80.611328125],\n",
       " 'get_approx_inputs_attn_8': [0.014303999952971935],\n",
       " 'get_approx_inputs_mlp_9': [90.95680236816406],\n",
       " 'get_approx_inputs_attn_9': [0.009279999881982803],\n",
       " 'get_approx_inputs_mlp_10': [100.81484985351562],\n",
       " 'get_approx_inputs_attn_10': [0.014336000196635723],\n",
       " 'get_approx_inputs_mlp_11': [110.74041748046875],\n",
       " 'get_approx_inputs_attn_11': [0.011264000087976456],\n",
       " 'get_approx_inputs': [667.2266845703125],\n",
       " 'approx_encode_mlp_0': [1.159168004989624],\n",
       " 'approx_decode_mlp_0': [0.5294079780578613],\n",
       " 'approx_forward_mlp_0': [2.2145919799804688],\n",
       " 'approx_encode_attn_0': [1.3878079652786255],\n",
       " 'approx_decode_attn_0': [0.5437440276145935],\n",
       " 'approx_forward_attn_0': [2.4187519550323486],\n",
       " 'approx_encode_mlp_1': [1.142848014831543],\n",
       " 'approx_decode_mlp_1': [0.5293759703636169],\n",
       " 'approx_forward_mlp_1': [2.1449599266052246],\n",
       " 'approx_encode_attn_1': [1.1454399824142456],\n",
       " 'approx_decode_attn_1': [0.5375999808311462],\n",
       " 'approx_forward_attn_1': [2.1454079151153564],\n",
       " 'approx_encode_mlp_2': [1.141055941581726],\n",
       " 'approx_decode_mlp_2': [0.5283839702606201],\n",
       " 'approx_forward_mlp_2': [2.1302719116210938],\n",
       " 'approx_encode_attn_2': [1.3160640001296997],\n",
       " 'approx_decode_attn_2': [0.5345280170440674],\n",
       " 'approx_forward_attn_2': [2.326143980026245],\n",
       " 'approx_encode_mlp_3': [1.1440320014953613],\n",
       " 'approx_decode_mlp_3': [0.5375999808311462],\n",
       " 'approx_forward_mlp_3': [2.149696111679077],\n",
       " 'approx_encode_attn_3': [1.1439039707183838],\n",
       " 'approx_decode_attn_3': [0.5283839702606201],\n",
       " 'approx_forward_attn_3': [2.1433279514312744],\n",
       " 'approx_encode_mlp_4': [1.1406079530715942],\n",
       " 'approx_decode_mlp_4': [0.5273600220680237],\n",
       " 'approx_forward_mlp_4': [2.161760091781616],\n",
       " 'approx_encode_attn_4': [1.3098560571670532],\n",
       " 'approx_decode_attn_4': [0.5314559936523438],\n",
       " 'approx_forward_attn_4': [2.312511920928955],\n",
       " 'approx_encode_mlp_5': [1.1490880250930786],\n",
       " 'approx_decode_mlp_5': [0.5294079780578613],\n",
       " 'approx_forward_mlp_5': [2.151456117630005],\n",
       " 'approx_encode_attn_5': [1.1509439945220947],\n",
       " 'approx_decode_attn_5': [0.5304319858551025],\n",
       " 'approx_forward_attn_5': [2.148319959640503],\n",
       " 'approx_encode_mlp_6': [1.1412160396575928],\n",
       " 'approx_decode_mlp_6': [0.5283839702606201],\n",
       " 'approx_forward_mlp_6': [2.1478400230407715],\n",
       " 'approx_encode_attn_6': [1.3183039426803589],\n",
       " 'approx_decode_attn_6': [0.5335040092468262],\n",
       " 'approx_forward_attn_6': [2.3222079277038574],\n",
       " 'approx_encode_mlp_7': [1.1432000398635864],\n",
       " 'approx_decode_mlp_7': [0.5263360142707825],\n",
       " 'approx_forward_mlp_7': [2.142848014831543],\n",
       " 'approx_encode_attn_7': [1.136191964149475],\n",
       " 'approx_decode_attn_7': [0.5334720015525818],\n",
       " 'approx_forward_attn_7': [2.127808094024658],\n",
       " 'approx_encode_mlp_8': [1.140544056892395],\n",
       " 'approx_decode_mlp_8': [0.5335040092468262],\n",
       " 'approx_forward_mlp_8': [2.157952070236206],\n",
       " 'approx_encode_attn_8': [1.3057279586791992],\n",
       " 'approx_decode_attn_8': [0.5335040092468262],\n",
       " 'approx_forward_attn_8': [2.3076159954071045],\n",
       " 'approx_encode_mlp_9': [1.146399974822998],\n",
       " 'approx_decode_mlp_9': [0.5273600220680237],\n",
       " 'approx_forward_mlp_9': [2.138240098953247],\n",
       " 'approx_encode_attn_9': [1.1554880142211914],\n",
       " 'approx_decode_attn_9': [0.5283839702606201],\n",
       " 'approx_forward_attn_9': [2.1574718952178955],\n",
       " 'approx_encode_mlp_10': [1.1398080587387085],\n",
       " 'approx_decode_mlp_10': [0.5263360142707825],\n",
       " 'approx_forward_mlp_10': [2.1217920780181885],\n",
       " 'approx_encode_attn_10': [1.3174079656600952],\n",
       " 'approx_decode_attn_10': [0.5335040092468262],\n",
       " 'approx_forward_attn_10': [2.314336061477661],\n",
       " 'approx_encode_mlp_11': [1.137760043144226],\n",
       " 'approx_decode_mlp_11': [0.5274239778518677],\n",
       " 'approx_forward_mlp_11': [2.123744010925293],\n",
       " 'approx_encode_attn_11': [1.1387840509414673],\n",
       " 'approx_decode_attn_11': [0.5314559936523438],\n",
       " 'approx_forward_attn_11': [2.1368319988250732],\n",
       " 'approx_forward': [53.77369689941406],\n",
       " 'total_loss': [821.5941162109375]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
