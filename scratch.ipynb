{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import IPython\n",
    "IPython.get_ipython().run_line_magic('cd', '..')  # Go up one directory\n",
    "\n",
    "from dictionary_learning.buffer import AllActivationBuffer\n",
    "from dictionary_learning.trainers.top_k import TrainerSCAE, AutoEncoderTopK\n",
    "from dictionary_learning.training import trainSCAE\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch as t\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if t.cuda.is_available() else \"cpu\"\n",
    "model = LanguageModel(\"gpt2\", device_map=device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "dataset = load_dataset(\n",
    "    'Skylion007/openwebtext', \n",
    "    split='train', \n",
    "    streaming=True,\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "\n",
    "class CustomData():\n",
    "    def __init__(self, dataset):\n",
    "        self.data = iter(dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.data)['text']\n",
    "\n",
    "data = CustomData(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "n_layer = model.config.n_layer\n",
    "\n",
    "submodule_list = [\n",
    "    (model.transformer.h[layer].mlp, \"in_and_out\")\n",
    "    for layer in range(n_layer)\n",
    "    ] \n",
    "submodule_list += [\n",
    "    (model.transformer.h[layer].attn, \"out\")\n",
    "     for layer in range(n_layer)\n",
    "     ]\n",
    "\n",
    "batch_size_tokens = 1024\n",
    "\n",
    "buffer = AllActivationBuffer(\n",
    "    data=data,\n",
    "    model=model,\n",
    "    submodules=submodule_list,\n",
    "    d_submodule=768, # output dimension of the model component\n",
    "    n_ctxs=128,  # you can set this higher or lower depending on your available memory\n",
    "    device=\"cuda\",\n",
    "    out_batch_size = batch_size_tokens,\n",
    "    refresh_batch_size = 256,\n",
    ")  # buffer will yield batches of tensors of dimension = num_submodules * d_submodule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get the configuration from the buffer to set up our trainer properly\n",
    "submodules = [submod for submod, _ in buffer.submodules]\n",
    "activation_dims = [buffer.d_submodule[submod] for submod in submodules]\n",
    "submodule_names = [submod.__class__.__name__ for submod in submodules]\n",
    "\n",
    "# Configure the multi-trainer\n",
    "trainer_cfg = {\n",
    "    \"trainer\": TrainerSCAE,\n",
    "    \"activation_dims\": activation_dims,\n",
    "    \"dict_sizes\": [dim * 16 for dim in activation_dims],\n",
    "    \"ks\": [128 for _ in submodules],  \n",
    "    \"device\": buffer.device,\n",
    "    \"layers\": list(range(len(submodules))),  # TODO: might want to fix this\n",
    "    \"lm_name\": \"your_model_name\",  # replace with actual model name\n",
    "    \"submodule_names\": submodule_names,\n",
    "}\n",
    "# Initialize trainer\n",
    "trainer_class = trainer_cfg.pop(\"trainer\")\n",
    "trainer = trainer_class(**trainer_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, targets = next(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(135047.2344, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.loss(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjacobcd52\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/wandb/run-20241128_210700-xrswh6og</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jacobcd52/sae_training/runs/xrswh6og' target=\"_blank\">vocal-dragon-8</a></strong> to <a href='https://wandb.ai/jacobcd52/sae_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jacobcd52/sae_training' target=\"_blank\">https://wandb.ai/jacobcd52/sae_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jacobcd52/sae_training/runs/xrswh6og' target=\"_blank\">https://wandb.ai/jacobcd52/sae_training/runs/xrswh6og</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 19\u001b[0m\n\u001b[1;32m      7\u001b[0m trainer_cfg \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainer\u001b[39m\u001b[38;5;124m\"\u001b[39m: TrainerSCAE,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation_dims\u001b[39m\u001b[38;5;124m\"\u001b[39m: activation_dims,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmodule_names\u001b[39m\u001b[38;5;124m\"\u001b[39m: submodule_names,\n\u001b[1;32m     16\u001b[0m }\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Run the training\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtrainSCAE\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msae_checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to False if you don't want to use wandb\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary_learning/training.py:218\u001b[0m, in \u001b[0;36mtrainSCAE\u001b[0;34m(buffer, trainer_cfg, steps, save_steps, save_dir, log_steps, use_wandb)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Log statistics\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m%\u001b[39m log_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 218\u001b[0m     loss_log \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     log_dict \u001b[38;5;241m=\u001b[39m loss_log\u001b[38;5;241m.\u001b[39mlosses\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# Add additional metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/dictionary_learning/trainers/top_k.py:533\u001b[0m, in \u001b[0;36mTrainerSCAE.loss\u001b[0;34m(self, xs, step, logging)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;124;03mTODO\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# First pass: get vanilla feature acts and losses.\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m vanilla_feature_acts, vanilla_l2_loss, vanilla_auxk_loss, vanilla_individual_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_forward_vanilla\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Use the vanilla feature_acts to get the approximate inputs\u001b[39;00m\n\u001b[1;32m    536\u001b[0m approx_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_approx_inputs(xs)\n",
      "File \u001b[0;32m~/dictionary_learning/trainers/top_k.py:424\u001b[0m, in \u001b[0;36mTrainerSCAE.run_forward_vanilla\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m    420\u001b[0m vanilla_feature_acts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x, ae) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(xs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maes)):\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# Run the SAE\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m     f, top_acts, top_indices \u001b[38;5;241m=\u001b[39m \u001b[43mae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_topk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     x_hat \u001b[38;5;241m=\u001b[39m ae\u001b[38;5;241m.\u001b[39mdecode(f)\n\u001b[1;32m    426\u001b[0m     vanilla_feature_acts\u001b[38;5;241m.\u001b[39mappend(f)\n",
      "File \u001b[0;32m~/dictionary_learning/trainers/top_k.py:74\u001b[0m, in \u001b[0;36mAutoEncoderTopK.encode\u001b[0;34m(self, x, return_topk)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: t\u001b[38;5;241m.\u001b[39mTensor, return_topk: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;66;03m# vanilla forward pass\u001b[39;00m\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;66;03m# x has shape [batch, d]\u001b[39;00m\n\u001b[1;32m     77\u001b[0m         preact_BF \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_dec)\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;66;03m# split forward pass per feature\u001b[39;00m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;66;03m# x has shape [batch, feature, d]\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# First get the configuration from the buffer to set up our trainer properly\n",
    "submodules = [submod for submod, _ in buffer.submodules]\n",
    "activation_dims = [buffer.d_submodule[submod] for submod in submodules]\n",
    "submodule_names = [submod.__class__.__name__ for submod in submodules]\n",
    "\n",
    "# Configure the multi-trainer\n",
    "trainer_cfg = {\n",
    "    \"trainer\": TrainerSCAE,\n",
    "    \"activation_dims\": activation_dims,\n",
    "    \"dict_sizes\": [dim * 16 for dim in activation_dims],\n",
    "    \"ks\": [128 for _ in submodules],  \n",
    "    \"device\": buffer.device,\n",
    "    \"layers\": list(range(len(submodules))),  # TODO: might want to fix this\n",
    "    \"lm_name\": \"your_model_name\",  # replace with actual model name\n",
    "    \"submodule_names\": submodule_names,\n",
    "}\n",
    "\n",
    "# Run the training\n",
    "trainer = trainSCAE(\n",
    "    buffer=buffer,\n",
    "    trainer_cfg=trainer_cfg,\n",
    "    steps=30000,\n",
    "    save_steps=1000,\n",
    "    save_dir=\"sae_checkpoints\",\n",
    "    log_steps=100,\n",
    "    use_wandb=True  # Set to False if you don't want to use wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
