{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from buffer import AllActivationBuffer\n",
    "from trainers.top_k import TrainerSCAE, AutoEncoderTopK\n",
    "from trainers.scae import SCAESuite, TrainerSCAESuite, TrainerConfig\n",
    "from training import train_scae_suite\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch as t\n",
    "from nnsight import LanguageModel\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "DTYPE = t.bfloat16\n",
    "device = \"cuda:0\" if t.cuda.is_available() else \"cpu\"\n",
    "model = LanguageModel(\"gpt2\", device_map=device, torch_dtype=DTYPE)\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "dataset = load_dataset(\n",
    "    'Skylion007/openwebtext', \n",
    "    split='train', \n",
    "    streaming=True,\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "\n",
    "class CustomData():\n",
    "    def __init__(self, dataset):\n",
    "        self.data = iter(dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.data)['text']\n",
    "\n",
    "data = CustomData(dataset)\n",
    "\n",
    "C = 10\n",
    "expansion = 16\n",
    "k = 128 # TODO: automatically detect these\n",
    "\n",
    "num_features = model.config.n_embd * expansion\n",
    "\n",
    "\n",
    "##\n",
    "n_layer = model.config.n_layer\n",
    "\n",
    "submodules = {}\n",
    "for layer in range(n_layer):\n",
    "    submodules[f\"mlp_{layer}\"] = (model.transformer.h[layer].mlp, \"in_and_out\")\n",
    "    submodules[f\"attn_{layer}\"] = (model.transformer.h[layer].attn, \"out\")\n",
    "\n",
    "def random_up_features(down_layer):\n",
    "    # Fake important-connection dictionary, for testing\n",
    "    dic = {}\n",
    "    for layer in range(down_layer):\n",
    "        dic[f\"mlp_{layer}\"]  = t.randint(0, num_features, (num_features, C), dtype=t.long)\n",
    "        dic[f\"attn_{layer}\"] = t.randint(0, num_features, (num_features, C), dtype=t.long)\n",
    "    return dic\n",
    "\n",
    "def all_up_features(down_layer):\n",
    "    # Fake important-connection dictionary, for testing\n",
    "    dic = {}\n",
    "    for layer in range(n_layer):\n",
    "        dic[f\"mlp_{layer}\"]  = t.randint(0, num_features, (num_features, C), dtype=t.long)\n",
    "        dic[f\"attn_{layer}\"] = t.randint(0, num_features, (num_features, C), dtype=t.long)\n",
    "    return dic\n",
    "\n",
    "important_features = {f\"mlp_{down_layer}\": random_up_features(down_layer) \n",
    "                      for down_layer in range(n_layer)}\n",
    "# important_features = {}\n",
    "                        \n",
    "\n",
    "# Get submodule names from the submodules dictionary\n",
    "submodule_names = list(submodules.keys())\n",
    "\n",
    "pretrained_info = {}\n",
    "for layer in range(n_layer):\n",
    "    for module in ['attn', 'mlp']:\n",
    "        pretrained_info[f'{module}_{layer}'] = {'repo_id': 'jacobcd52/scae', 'filename': f'ae_{module}_{layer}.pt'}\n",
    "\n",
    "\n",
    "##\n",
    "buffer = AllActivationBuffer(\n",
    "    data=data,\n",
    "    model=model,\n",
    "    submodules=submodules,\n",
    "    d_submodule=model.config.n_embd, # output dimension of the model component\n",
    "    n_ctxs=128,  # you can set this higher or lower depending on your available memory\n",
    "    device=\"cuda\",\n",
    "    out_batch_size = 64,\n",
    "    refresh_batch_size = 256,\n",
    "    dtype=DTYPE,\n",
    ") \n",
    "\n",
    "\n",
    "# def run_evaluation(trainer, buffer, n_batches=100, ce_batch_size=32):\n",
    "#     varexp_metrics = {name : {} for name in trainer.submodules.keys()}\n",
    "#     ce_metrics = {name : {} for name in trainer.submodules.keys()}\n",
    "#     for i in tqdm(range(n_batches)):\n",
    "#         # get varexp metrics\n",
    "#         input_acts, output_acts = next(buffer)\n",
    "#         batch_varexp_metrics = trainer.evaluate_varexp_batch(input_acts, output_acts, use_sparse_connections=False)\n",
    "\n",
    "#         # get CE metrics\n",
    "#         b = buffer.refresh_batch_size\n",
    "#         buffer.refresh_batch_size = ce_batch_size\n",
    "#         tokens = buffer.token_batch()\n",
    "#         batch_ce_metrics = trainer.evaluate_ce_batch(model, tokens, use_sparse_connections=False)\n",
    "#         buffer.refresh_batch_size = b\n",
    "\n",
    "#         for name in ce_metrics.keys():\n",
    "#             for metric in batch_ce_metrics[name].keys():\n",
    "#                 ce_metrics[name][metric] = ce_metrics[name].get(metric, 0) + batch_ce_metrics[name][metric]\n",
    "#             for metric in batch_varexp_metrics[name].keys():\n",
    "#                 varexp_metrics[name][metric] = varexp_metrics[name].get(metric, 0) + batch_varexp_metrics[name][metric]\n",
    "    \n",
    "#     for name in ce_metrics.keys():\n",
    "#         for metric in ce_metrics[name].keys():\n",
    "#             ce_metrics[name][metric] = ce_metrics[name][metric] / n_batches\n",
    "#         for metric in varexp_metrics[name].keys():\n",
    "#             varexp_metrics[name][metric] = varexp_metrics[name][metric] / n_batches\n",
    "        \n",
    "#     return varexp_metrics, ce_metrics\n",
    "\n",
    "# varexp_metrics, ce_metrics = run_evaluation(trainer, buffer, n_batches=2, ce_batch_size=32)\n",
    "\n",
    "# print(f\"Clean loss = {ce_metrics['mlp_0']['loss_original']:.3f}\")\n",
    "# print()\n",
    "# for name in [k for k in ce_metrics.keys() if 'mlp' in k]:\n",
    "#     print(f\"{name}   {ce_metrics[name]['loss_reconstructed'] - ce_metrics[name]['loss_original']:.3f}   {varexp_metrics[name]['frac_variance_explained']*100:.0f}%     {varexp_metrics[name]['l2_loss']:.1f}\")\n",
    "\n",
    "# print()\n",
    "\n",
    "# for name in [k for k in ce_metrics.keys() if 'attn' in k]:\n",
    "#     print(f\"{name}   {ce_metrics[name]['loss_reconstructed'] - ce_metrics[name]['loss_original']:.3f}   {varexp_metrics[name]['frac_variance_explained']*100:.0f}%    {varexp_metrics[name]['l2_loss']:.1f}\")\n",
    "\n",
    "\n",
    "pretrained_configs = {}\n",
    "connections = defaultdict(dict)\n",
    "\n",
    "for down_layer in range(n_layer):\n",
    "    for module in ['attn', 'mlp']:\n",
    "        down_name = f'{module}_{down_layer}'\n",
    "        pretrained_configs[f'{module}_{down_layer}'] = {\n",
    "            'repo_id': 'jacobcd52/scae', \n",
    "            'filename': f'ae_{module}_{down_layer}.pt',\n",
    "            'k' : k\n",
    "            }\n",
    "        \n",
    "        # Use random connections for testing\n",
    "        for up_layer in range(down_layer):\n",
    "            up_name = f'{module}_{up_layer}'\n",
    "            connections[down_name][up_name] = t.randint(0, num_features, (num_features, C), dtype=t.long)\n",
    "\n",
    "suite = SCAESuite.from_pretrained(pretrained_configs, connections=connections)\n",
    "\n",
    "trainer_cfg = TrainerConfig(\n",
    "    connection_sparsity_coeff=0.1,\n",
    ")\n",
    "\n",
    "trainer = train_scae_suite(\n",
    "    buffer,\n",
    "    module_specs=pretrained_configs,\n",
    "    trainer_config=trainer_cfg,\n",
    "    connections=connections,\n",
    "    steps=100,\n",
    "    save_steps = 10,\n",
    "    # save_dir: Optional[str] = None,\n",
    "    # log_steps: Optional[int] = None,\n",
    "    # use_wandb: bool = False,\n",
    "    # hf_repo_id: Optional[str] = None,\n",
    "    dtype = DTYPE,\n",
    "    device=device,\n",
    "    # seed: Optional[int] = None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
