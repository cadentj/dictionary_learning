{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch as t\n",
    "from nnsight import LanguageModel\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from buffer import AllActivationBuffer\n",
    "from trainers.scae import SCAESuite\n",
    "\n",
    "DTYPE = t.bfloat16\n",
    "device = \"cuda:0\" if t.cuda.is_available() else \"cpu\"\n",
    "model = LanguageModel(\"gpt2\", device_map=device, torch_dtype=DTYPE)\n",
    "\n",
    "dataset = load_dataset(\n",
    "    'Skylion007/openwebtext', \n",
    "    split='train', \n",
    "    streaming=True,\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "\n",
    "class CustomData():\n",
    "    '''dumb helper class to make the dataset iterable'''\n",
    "    def __init__(self, dataset):\n",
    "        self.data = iter(dataset)\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        return next(self.data)['text']\n",
    "\n",
    "data = CustomData(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 10\n",
    "expansion = 16\n",
    "k = 128\n",
    "\n",
    "num_features = model.config.n_embd * expansion\n",
    "n_layer = model.config.n_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 4.2480e-02,  3.2715e-02,  4.4861e-03,  1.5747e-02,  5.9509e-03,\n",
       "        -2.6855e-02,  1.1719e-02,  5.1270e-02,  1.3245e-02, -4.2419e-03,\n",
       "        -1.1658e-02,  2.1973e-03, -1.3367e-02, -2.0599e-03, -2.6489e-02,\n",
       "         1.1902e-02, -4.2419e-03, -1.7822e-02,  4.7852e-02, -3.5553e-03,\n",
       "         1.0490e-04, -1.2268e-02,  1.0864e-02, -5.9509e-03,  6.3171e-03,\n",
       "         5.8289e-03,  6.2012e-02,  7.7057e-04, -2.4567e-03,  3.7994e-03,\n",
       "         4.1504e-03,  3.8574e-02,  7.1106e-03, -1.9775e-02, -8.7280e-03,\n",
       "         1.0559e-02,  1.3281e-01,  6.4087e-03,  8.8501e-03,  3.2715e-02,\n",
       "         1.2146e-02, -6.0730e-03,  1.4160e-02, -6.6833e-03,  3.1250e-02,\n",
       "        -2.2705e-02,  3.3936e-02, -1.2695e-02,  8.4473e-02,  3.5645e-02,\n",
       "         4.0894e-03, -4.0894e-03, -7.2266e-02,  6.3782e-03,  3.6621e-02,\n",
       "         5.6641e-02, -2.8839e-03,  7.2754e-02,  3.3875e-03, -1.8677e-02,\n",
       "         4.3701e-02, -8.9722e-03,  1.7242e-03,  4.8828e-03,  6.4844e-01,\n",
       "         2.2461e-02, -1.6098e-03,  4.7266e-01, -2.3682e-02, -1.0315e-02,\n",
       "         4.8828e-02,  4.1992e-02, -1.2512e-02, -2.5391e-02, -1.6479e-02,\n",
       "        -3.3691e-02,  2.1289e-01, -3.3936e-02, -2.1484e-02, -1.8677e-02,\n",
       "        -2.3041e-03,  4.5776e-03,  1.1475e-02, -4.2969e-02, -4.2480e-02,\n",
       "         4.4556e-03,  2.6611e-02, -1.0498e-01, -3.5645e-02,  8.1787e-03,\n",
       "         5.3406e-03, -4.3945e-03, -7.0312e-02, -2.7466e-02, -1.4038e-02,\n",
       "        -8.0566e-03, -1.0010e-02,  1.2451e-02, -4.7302e-03,  3.4424e-02,\n",
       "         2.5269e-02, -2.6733e-02,  5.2246e-02,  1.5564e-03,  7.2021e-03,\n",
       "        -1.0010e-02, -6.5918e-03,  9.7168e-02,  1.9836e-03, -7.4463e-03,\n",
       "         4.7119e-02,  4.2419e-03, -1.0010e-02,  3.2227e-02,  2.8564e-02,\n",
       "        -5.6458e-03,  1.5869e-02, -5.4932e-03, -2.4109e-03, -2.2217e-02,\n",
       "         1.9836e-03,  1.5198e-02, -2.3937e-04,  2.1973e-02,  7.4768e-03,\n",
       "         3.4790e-03, -1.8997e-03, -4.9561e-02,  5.4932e-02, -7.0801e-03,\n",
       "        -2.0020e-02, -1.0300e-03, -2.4780e-02, -2.1118e-02, -1.1902e-02,\n",
       "        -8.1787e-03,  1.3885e-03, -1.2817e-03,  3.0469e-01, -1.5991e-02,\n",
       "         5.2795e-03, -3.4027e-03, -5.7373e-02, -8.3618e-03, -8.7280e-03,\n",
       "         6.2500e-02,  4.0771e-02,  7.1335e-04,  5.5847e-03, -1.0010e-02,\n",
       "         5.7602e-04,  1.9238e-01,  2.0630e-02, -4.9133e-03,  1.6357e-02,\n",
       "         4.5471e-03, -1.3428e-02,  1.7456e-02,  3.9062e-02,  2.9419e-02,\n",
       "         1.2500e-01,  1.3489e-02,  4.6082e-03,  1.2939e-02,  6.2866e-03,\n",
       "        -1.0864e-02, -4.9438e-03,  4.8828e-02,  4.9744e-03, -4.8340e-02,\n",
       "        -2.3956e-03, -6.0425e-03, -2.4261e-03,  9.8267e-03, -4.9133e-03,\n",
       "         7.4158e-03,  1.8188e-02, -2.2339e-02,  5.2002e-02, -2.0996e-02,\n",
       "        -1.7944e-02,  4.1748e-02, -7.6294e-03,  5.8105e-02, -1.2817e-02,\n",
       "        -1.5381e-02,  1.4465e-02,  6.3171e-03,  1.4954e-03,  2.2095e-02,\n",
       "         1.2207e-02, -1.0400e-01,  1.1108e-02, -1.8433e-02,  7.3853e-03,\n",
       "        -2.4414e-02, -4.7302e-03, -1.9775e-02, -7.9346e-04,  1.5736e-04,\n",
       "         2.1057e-03,  1.9043e-02,  4.4250e-04,  4.1809e-03,  1.7334e-02,\n",
       "        -2.3956e-03,  1.1826e-03, -9.5215e-03,  8.2031e-02, -1.3123e-03,\n",
       "         1.0620e-02, -8.2520e-02,  1.1414e-02,  3.7598e-02,  4.3030e-03,\n",
       "         4.1771e-04, -3.9551e-02,  3.0060e-03,  7.2266e-02, -8.4839e-03,\n",
       "        -3.7994e-03, -1.9043e-02, -1.3062e-02,  2.8931e-02,  4.0283e-02,\n",
       "         1.8845e-03, -3.9368e-03, -3.2501e-03,  8.2397e-03, -2.2430e-03,\n",
       "         2.2888e-03, -3.6377e-02,  1.4551e-01, -3.7842e-02, -1.3245e-02,\n",
       "        -6.2561e-03,  9.9945e-04,  3.7842e-03,  6.3477e-02,  6.9580e-03,\n",
       "         3.0884e-02,  7.2266e-02,  1.9836e-03, -9.2163e-03, -9.1553e-04,\n",
       "         1.5015e-02,  2.1210e-03,  7.5684e-03, -1.4648e-02,  5.2979e-02,\n",
       "        -9.3262e-02,  9.2773e-03,  9.2163e-03, -6.0730e-03, -2.4261e-03,\n",
       "         2.1729e-02, -6.9336e-02,  1.1621e-01, -1.4648e-03,  3.4943e-03,\n",
       "         9.1553e-04,  1.4343e-02, -9.0942e-03, -4.8447e-04, -8.1177e-03,\n",
       "         8.4839e-03,  5.6250e-01,  1.8311e-02,  5.6763e-03, -2.4170e-02,\n",
       "        -1.6357e-02,  1.9897e-02, -6.5613e-03,  2.1118e-02,  6.0303e-02,\n",
       "        -2.0508e-02,  1.1169e-02, -3.2959e-02,  1.3062e-02, -1.2207e-02,\n",
       "         1.1658e-02, -9.0332e-03,  2.8076e-02, -5.1025e-02,  1.3672e-01,\n",
       "         1.7090e-02,  4.9744e-03,  8.7891e-03, -1.8164e-01, -8.1177e-03,\n",
       "        -6.2561e-03, -2.1515e-03, -1.3794e-02,  2.8320e-02, -6.0654e-04,\n",
       "        -1.9684e-03,  7.3547e-03,  2.6611e-02, -1.2634e-02,  6.7139e-04,\n",
       "        -2.0264e-02, -4.2969e-02,  1.0303e-01, -1.4709e-02, -1.5747e-02,\n",
       "         6.5308e-03,  1.4526e-02, -2.1729e-02,  4.3701e-02, -1.0315e-02,\n",
       "         1.1539e-04, -6.9824e-02, -1.0376e-02,  2.8198e-02, -3.2959e-02,\n",
       "        -1.4160e-02,  9.6436e-03,  1.5137e-02, -2.3041e-03, -9.7046e-03,\n",
       "         5.0293e-02,  8.9722e-03,  1.1597e-03,  2.8564e-02, -6.8054e-03,\n",
       "         1.7334e-02, -6.6406e-01, -1.2695e-02, -7.0801e-03, -1.1169e-02,\n",
       "         2.4780e-02,  1.0925e-02, -6.6223e-03, -1.3657e-03,  1.6113e-02,\n",
       "        -1.7456e-02, -5.5664e-02, -2.3560e-02, -2.6245e-02,  2.0508e-02,\n",
       "        -1.3123e-02,  4.9973e-04, -1.4343e-02,  5.3223e-02,  2.7466e-02,\n",
       "         7.9346e-03,  2.7466e-02,  9.8877e-03,  1.2634e-02,  1.1475e-02,\n",
       "         3.7537e-03,  4.7363e-02,  1.7456e-02, -1.0071e-02,  2.0996e-02,\n",
       "         1.0986e-02,  3.3691e-02,  4.7913e-03, -2.1240e-02,  1.3916e-02,\n",
       "         4.4678e-02, -2.6001e-02,  1.2012e-01, -5.6076e-04,  1.6846e-02,\n",
       "         1.8311e-02, -1.5747e-02, -4.3945e-03, -2.6703e-04, -3.4943e-03,\n",
       "         2.0752e-02,  9.4604e-03, -3.9307e-02,  7.3828e-01, -5.5420e-02,\n",
       "         9.5825e-03,  4.3945e-03,  1.1719e-02, -1.0205e-01,  4.1016e-02,\n",
       "        -1.1047e-02,  1.2436e-03,  2.1057e-03,  1.0449e-01, -3.0670e-03,\n",
       "         8.5449e-02,  4.4678e-02,  4.4861e-03, -1.5182e-03, -5.0049e-02,\n",
       "        -2.5177e-03, -1.4832e-02,  1.3062e-02, -1.7285e-01, -4.0436e-04,\n",
       "        -1.1353e-02,  5.5420e-02, -1.7471e-03, -6.8359e-03, -1.6113e-02,\n",
       "        -2.6733e-02, -4.0588e-03,  2.3560e-02, -5.9814e-03, -1.0254e-02,\n",
       "         1.1969e-04, -1.6235e-02, -1.0376e-02,  3.4668e-02, -8.9722e-03,\n",
       "        -2.3560e-02, -5.1880e-03, -2.0905e-03,  1.0986e-02,  6.4453e-02,\n",
       "         1.7212e-02, -4.3030e-03, -1.6846e-02, -3.4912e-02,  1.2451e-02,\n",
       "         8.4473e-02,  1.6357e-02,  5.0964e-03, -2.3438e-02, -1.4587e-02,\n",
       "         2.4536e-02,  4.9133e-03, -2.9602e-03, -2.2705e-02, -1.3672e-02,\n",
       "         6.6895e-02, -2.3193e-02, -2.5940e-03, -6.4697e-03, -8.2397e-03,\n",
       "        -3.4180e-03, -3.5858e-03, -2.2339e-02,  1.8066e-02,  4.2969e-02,\n",
       "         5.9326e-02,  3.3691e-02, -2.3499e-03,  9.6436e-03,  5.4932e-03,\n",
       "         3.8757e-03, -3.6377e-02, -5.0391e-01, -1.2634e-02,  1.5320e-02,\n",
       "         5.0537e-02, -2.2827e-02, -3.0060e-03,  1.0071e-02,  4.2725e-02,\n",
       "         5.9082e-02,  3.7109e-02,  8.9722e-03,  9.4986e-04,  1.3733e-02,\n",
       "         2.3315e-02,  1.3000e-02,  2.3560e-02,  2.3560e-02, -4.1504e-02,\n",
       "         2.8687e-03,  9.2773e-03,  1.3275e-03,  6.7902e-04,  1.1536e-02,\n",
       "        -1.1230e-02,  5.7983e-03,  2.4780e-02,  4.1199e-03, -4.3030e-03,\n",
       "        -1.7853e-03, -4.8828e-03, -6.4087e-03, -4.0039e-02, -4.5654e-02,\n",
       "         5.6641e-01,  7.7637e-02, -5.3101e-03,  1.6235e-02,  3.7109e-02,\n",
       "         1.2512e-02, -3.9673e-03,  2.5024e-03,  1.7944e-02, -2.1240e-02,\n",
       "         2.3071e-02, -2.7344e-02, -1.5259e-03,  1.0132e-02,  2.7588e-02,\n",
       "         8.3618e-03,  6.6895e-02,  4.4678e-02, -1.5137e-02, -4.0588e-03,\n",
       "        -7.5378e-03,  4.5776e-03,  4.8584e-02,  2.2461e-02,  7.1289e-02,\n",
       "        -1.7548e-03,  1.3062e-02,  3.9307e-02, -1.7944e-02,  1.4465e-02,\n",
       "         7.5989e-03,  5.3955e-02,  7.5195e-02, -2.4292e-02, -1.3962e-03,\n",
       "         8.0566e-02,  1.8799e-02, -6.3965e-02,  6.6833e-03,  4.5898e-02,\n",
       "         4.5776e-03, -3.6163e-03,  1.5991e-02, -2.3071e-02,  7.6599e-03,\n",
       "         2.9907e-02,  8.3496e-02,  4.4861e-03, -3.1738e-02, -6.8359e-03,\n",
       "         4.7119e-02,  1.9287e-02,  5.2185e-03,  9.7275e-04, -1.5747e-02,\n",
       "        -1.0559e-02, -1.0132e-02,  2.4872e-03,  8.5938e-02, -1.4725e-03,\n",
       "         1.3977e-02, -1.9531e-02,  2.4414e-03, -6.7871e-02,  4.6968e-05,\n",
       "         6.6223e-03, -3.2806e-03, -1.2451e-02, -1.2817e-02,  3.4668e-02,\n",
       "         6.4697e-03,  2.0996e-02,  1.2573e-02, -1.8555e-02,  1.5625e-01,\n",
       "         2.0508e-02,  7.1777e-02, -8.7891e-03, -8.7891e-03, -1.9775e-02,\n",
       "         4.0527e-02, -1.0132e-02, -4.5410e-02, -1.0986e-02,  1.1414e-02,\n",
       "         4.9316e-02,  4.7607e-02, -3.7598e-02,  6.2012e-02,  2.6489e-02,\n",
       "        -2.2339e-02,  1.4465e-02,  1.8555e-02,  4.8828e-03, -1.5259e-02,\n",
       "         1.9922e-01, -7.7820e-03, -1.6357e-02, -1.7456e-02,  1.0010e-02,\n",
       "        -2.1118e-02, -1.7944e-02,  9.2316e-04, -6.8054e-03,  1.0803e-02,\n",
       "         1.9287e-02,  2.4414e-02,  3.9551e-02,  5.0293e-02,  6.9336e-02,\n",
       "        -8.3008e-03, -2.1240e-02, -4.7607e-03, -1.2512e-03,  1.0437e-02,\n",
       "        -1.3306e-02,  6.5308e-03, -1.0864e-02, -1.9409e-02, -1.1353e-02,\n",
       "        -5.7617e-02,  8.5449e-03, -4.3457e-02,  3.0640e-02,  3.4180e-02,\n",
       "         1.4343e-02,  1.2207e-02,  2.5630e-05,  9.6191e-02,  1.4709e-02,\n",
       "        -3.2715e-02,  3.4912e-02,  8.1787e-03, -5.6458e-03, -1.3367e-02,\n",
       "         2.4170e-02,  5.5420e-02,  1.3855e-02, -4.9072e-02,  8.6670e-03,\n",
       "        -1.1182e-01,  1.8921e-02, -2.8687e-03, -6.7383e-02, -5.6396e-02,\n",
       "        -5.5420e-02,  9.0790e-04, -9.0942e-03, -1.0529e-03, -1.0986e-02,\n",
       "        -2.0508e-02, -5.6839e-04,  1.7822e-02, -2.9297e-03, -5.4321e-03,\n",
       "        -1.3657e-03, -4.0039e-02,  1.2283e-03,  1.9150e-03,  1.7090e-02,\n",
       "         1.1865e-01,  7.7820e-03,  4.8584e-02, -8.8867e-02, -1.2054e-03,\n",
       "        -2.0142e-02, -1.7700e-02,  2.5513e-02,  3.3691e-02, -4.1016e-02,\n",
       "        -3.7537e-03,  8.5449e-02,  6.9580e-03,  6.9336e-02, -2.1667e-03,\n",
       "         4.6143e-02, -7.1777e-02,  3.1982e-02, -2.4780e-02,  4.4189e-02,\n",
       "         1.3428e-02, -2.0996e-02,  6.7139e-03,  4.7913e-03, -1.2878e-02,\n",
       "         4.3945e-02, -1.0071e-02,  2.8198e-02,  7.5684e-02,  2.8442e-02,\n",
       "        -1.2695e-02,  1.2268e-02, -9.9487e-03, -2.3438e-02,  4.8218e-03,\n",
       "         2.2125e-03,  1.2741e-03, -1.6113e-02,  7.7148e-02,  4.2236e-02,\n",
       "        -3.8330e-02, -1.9775e-02, -3.5667e-04,  2.3682e-02, -3.9368e-03,\n",
       "         7.5684e-03,  8.3618e-03, -1.1963e-02,  5.6152e-02,  1.7700e-02,\n",
       "        -2.1057e-03,  2.4658e-02, -4.1809e-03,  2.0386e-02, -1.6235e-02,\n",
       "        -1.4343e-02,  7.5378e-03, -4.5776e-03, -2.1362e-03,  5.8594e-02,\n",
       "         1.2817e-02,  2.4658e-02,  6.6833e-03, -3.7842e-02, -6.0120e-03,\n",
       "        -1.8234e-03,  1.6357e-02, -5.3406e-03,  8.7891e-03, -1.7452e-04,\n",
       "         5.5847e-03, -1.8799e-02, -4.6387e-03, -3.9062e-03, -2.1240e-02,\n",
       "         1.1902e-02,  1.0620e-02,  1.0205e-01,  8.7402e-02,  1.9653e-02,\n",
       "         8.3984e-02, -3.6621e-02, -4.8828e-03,  7.8964e-04,  2.6367e-02,\n",
       "         1.0193e-02,  7.9346e-03,  6.1340e-03,  1.7700e-02,  6.5002e-03,\n",
       "         3.4790e-03, -4.0527e-02, -1.8799e-02, -3.3447e-02,  1.4893e-02,\n",
       "         1.9897e-02, -2.3315e-02,  1.0132e-02,  1.4221e-02, -2.0410e-01,\n",
       "        -1.1292e-02, -9.9487e-03,  2.1118e-02, -2.0142e-02,  1.4258e-01,\n",
       "         3.5352e-01, -2.4658e-02,  1.3367e-02, -9.8267e-03,  3.0884e-02,\n",
       "        -1.0742e-02, -4.4922e-02,  1.0529e-03,  1.4893e-02, -1.5625e-02,\n",
       "         1.1353e-02,  2.5391e-01,  2.1606e-02, -1.3184e-02,  3.8330e-02,\n",
       "         7.0190e-03, -1.4687e-04, -3.3417e-03, -1.8387e-03,  1.2024e-02,\n",
       "        -1.7822e-02,  6.3477e-03, -1.6479e-03], device='cuda:0',\n",
       "       dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].ln_2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/dictionary_learning/notebooks/../trainers/scae.py:572: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = t.load(weights_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "pretrained_configs = {}\n",
    "connections = defaultdict(dict)\n",
    "\n",
    "for down_layer in range(n_layer):\n",
    "    for module in ['attn', 'mlp']:\n",
    "        down_name = f'{module}_{down_layer}'\n",
    "        pretrained_configs[f'{module}_{down_layer}'] = {\n",
    "            'repo_id': 'jacobcd52/scae', \n",
    "            'filename': f'ae_{module}_{down_layer}.pt',\n",
    "            'k' : k,\n",
    "            'layernorm_gamma': 100\n",
    "            }\n",
    "        \n",
    "        # Use random connections for testing\n",
    "        if module=='mlp':\n",
    "            for up_layer in range(down_layer+1):\n",
    "                up_name = f'{module}_{up_layer}'\n",
    "                connections[down_name][up_name] = t.randint(0, num_features, (num_features, C), dtype=t.long)\n",
    "\n",
    "suite = SCAESuite.from_pretrained(pretrained_configs, connections=connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/dictionary_learning/notebooks/../buffer.py:606: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with t.cuda.amp.autocast(dtype=self.dtype):\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "submodules = {}\n",
    "for layer in range(n_layer):\n",
    "    submodules[f\"mlp_{layer}\"] = (model.transformer.h[layer].mlp, \"in_and_out\")\n",
    "    submodules[f\"attn_{layer}\"] = (model.transformer.h[layer].attn, \"out\")\n",
    "    \n",
    "buffer = AllActivationBuffer(\n",
    "    data=data,\n",
    "    model=model,\n",
    "    submodules=submodules,\n",
    "    d_submodule=model.config.n_embd,\n",
    "    n_ctxs=128,\n",
    "    out_batch_size = 32,\n",
    "    refresh_batch_size = 256,\n",
    "    device=device,\n",
    "    dtype=DTYPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(\n",
    "        suite, \n",
    "        buffer, \n",
    "        n_batches=10, \n",
    "        ce_batch_size=32,\n",
    "        use_sparse_connections=False\n",
    "        ):\n",
    "    '''Simple function to run evaluation on several batches, and return the average metrics'''\n",
    "    \n",
    "    varexp_metrics = {name : {} for name in buffer.submodules.keys()}\n",
    "    ce_metrics = {name : {} for name in buffer.submodules.keys()}\n",
    "\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        # get varexp metrics\n",
    "        input_acts, output_acts = next(buffer)\n",
    "        batch_varexp_metrics = suite.evaluate_varexp_batch(\n",
    "            input_acts, \n",
    "            output_acts,\n",
    "            use_sparse_connections=use_sparse_connections\n",
    "            )\n",
    "\n",
    "        # get CE metrics\n",
    "        b = buffer.refresh_batch_size\n",
    "        buffer.refresh_batch_size = ce_batch_size\n",
    "        tokens = buffer.token_batch()\n",
    "        batch_ce_metrics = suite.evaluate_ce_batch(\n",
    "            model, \n",
    "            tokens, \n",
    "            buffer.submodules,\n",
    "            use_sparse_connections=use_sparse_connections\n",
    "            )\n",
    "        buffer.refresh_batch_size = b\n",
    "\n",
    "        for name in ce_metrics.keys():\n",
    "            for metric in batch_ce_metrics[name].keys():\n",
    "                ce_metrics[name][metric] = ce_metrics[name].get(metric, 0) + batch_ce_metrics[name][metric] / n_batches\n",
    "            for metric in batch_varexp_metrics[name].keys():\n",
    "                varexp_metrics[name][metric] = varexp_metrics[name].get(metric, 0) + batch_varexp_metrics[name][metric] / n_batches\n",
    "           \n",
    "    return varexp_metrics, ce_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.97s/it]\n"
     ]
    }
   ],
   "source": [
    "varexp_metrics, ce_metrics = run_evaluation(\n",
    "    suite, \n",
    "    buffer, \n",
    "    n_batches=1, \n",
    "    ce_batch_size=32,\n",
    "    use_sparse_connections=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean loss = 3.438\n",
      "\n",
      "Module  CE increase  CE expl Var expl\n",
      "mlp_0   7.625        -77%     -1670532%\n",
      "mlp_1   7.875        -12500%     -3465992%\n",
      "mlp_2   4.531        -7150%     -1822076%\n",
      "mlp_3   14.438        -18380%     -2973179%\n",
      "mlp_4   14.188        -22600%     -2956593%\n",
      "mlp_5   14.938        -19020%     -1985468%\n",
      "mlp_6   16.312        -17300%     -1583495%\n",
      "mlp_7   17.438        -18500%     -1205486%\n",
      "mlp_8   17.562        -18633%     -729978%\n",
      "mlp_9   16.062        -14586%     -474382%\n",
      "mlp_10   11.000        -8700%     -174055%\n",
      "mlp_11   7.375        -3531%     -31672%\n",
      "\n",
      "attn_0   0.016        99%     99%\n",
      "attn_1   0.000        100%     96%\n",
      "attn_2   0.016        67%     96%\n",
      "attn_3   0.000        100%     93%\n",
      "attn_4   0.016        67%     92%\n",
      "attn_5   0.000        100%     92%\n",
      "attn_6   0.000        100%     91%\n",
      "attn_7   0.000        100%     93%\n",
      "attn_8   0.000        100%     92%\n",
      "attn_9   0.016        80%     93%\n",
      "attn_10   0.016        75%     93%\n",
      "attn_11   0.016        92%     99%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Clean loss = {ce_metrics['mlp_0']['loss_original']:.3f}\\n\")\n",
    "\n",
    "print(\"Module  CE increase  CE expl Var expl\")\n",
    "for name in [k for k in ce_metrics.keys() if 'mlp' in k]:\n",
    "    print(f\"{name}   {ce_metrics[name]['loss_reconstructed'] - ce_metrics[name]['loss_original']:.3f}        {ce_metrics[name]['frac_recovered']*100:.0f}%     {varexp_metrics[name]['frac_variance_explained']*100:.0f}%\")\n",
    "\n",
    "print()\n",
    "\n",
    "for name in [k for k in ce_metrics.keys() if 'attn' in k]:\n",
    "    print(f\"{name}   {ce_metrics[name]['loss_reconstructed'] - ce_metrics[name]['loss_original']:.3f}        {ce_metrics[name]['frac_recovered']*100:.0f}%     {varexp_metrics[name]['frac_variance_explained']*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attn_0': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'mlp_0': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'attn_1': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'mlp_1': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'attn_2': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'mlp_2': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'attn_3': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'mlp_3': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'attn_4': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'mlp_4': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'attn_5': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'mlp_5': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'attn_6': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'mlp_6': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'attn_7': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'mlp_7': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'attn_8': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'mlp_8': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'attn_9': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'mlp_9': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'attn_10': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'mlp_10': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'attn_11': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0),\n",
       " 'mlp_11': SubmoduleConfig(activation_dim=768, dict_size=12288, k=128, upstream_connections=None, layernorm_gamma=1.0)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suite.configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
