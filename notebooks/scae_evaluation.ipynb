{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch as t\n",
    "from nnsight import LanguageModel\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "from buffer import AllActivationBuffer\n",
    "from trainers.scae import SCAESuite\n",
    "from utils import load_model_with_folded_ln2, load_iterable_dataset\n",
    "\n",
    "DTYPE = t.float32\n",
    "device = \"cuda:0\" if t.cuda.is_available() else \"cpu\"\n",
    "t.manual_seed(42)\n",
    "\n",
    "model = load_model_with_folded_ln2(\"gpt2\", device=device, torch_dtype=DTYPE)\n",
    "data = load_iterable_dataset('Skylion007/openwebtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 10\n",
    "expansion = 16\n",
    "k = 128\n",
    "\n",
    "num_features = model.config.n_embd * expansion\n",
    "n_layer = model.config.n_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/dictionary_learning/notebooks/../trainers/scae.py:627: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = t.load(checkpoint_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "suite = SCAESuite.from_pretrained(\n",
    "    'jacobcd52/gpt2_suite_folded_ln',\n",
    "    device=device,\n",
    "    dtype=DTYPE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "initial_submodule = model.transformer.h[0]\n",
    "layernorm_submodules = {}\n",
    "submodules = {}\n",
    "for layer in range(n_layer):\n",
    "    submodules[f\"mlp_{layer}\"] = (model.transformer.h[layer].mlp, \"in_and_out\")\n",
    "    submodules[f\"attn_{layer}\"] = (model.transformer.h[layer].attn, \"out\")\n",
    "\n",
    "    layernorm_submodules[f\"mlp_{layer}\"] = model.transformer.h[layer].ln_2\n",
    "\n",
    "buffer = AllActivationBuffer(\n",
    "    data=data,\n",
    "    model=model,\n",
    "    submodules=submodules,\n",
    "    initial_submodule=initial_submodule,\n",
    "    layernorm_submodules=layernorm_submodules,\n",
    "    d_submodule=model.config.n_embd,\n",
    "    n_ctxs=128,\n",
    "    out_batch_size = 32,\n",
    "    refresh_batch_size = 256,\n",
    "    device=device,\n",
    "    dtype=DTYPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load connections from top_connections.pkl\n",
    "# # USE PICKLE\n",
    "# import pickle\n",
    "# with open('/root/dictionary_learning/top_connections.pkl', 'rb') as f:\n",
    "#     top_connections = pickle.load(f)\n",
    "# suite.connections = top_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conns = t.arange(0, num_features, device=device, dtype=t.int64).expand(num_features, num_features)\n",
    "connections = {}\n",
    "for down_layer in range(n_layer):\n",
    "    # connections[f\"attn_{down_layer}\"] = {}\n",
    "    connections[f\"mlp_{down_layer}\"] = {}\n",
    "    for up_layer in range(down_layer+1):\n",
    "        connections[f\"mlp_{down_layer}\"][f\"attn_{up_layer}\"] = all_conns\n",
    "        connections[f\"mlp_{down_layer}\"][f\"mlp_{up_layer}\"] = all_conns\n",
    "\n",
    "suite.connections = connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12288, 12288])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_conns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(\n",
    "        suite, \n",
    "        buffer, \n",
    "        n_batches=10, \n",
    "        ce_batch_size=32,\n",
    "        use_sparse_connections=False\n",
    "        ):\n",
    "    '''Simple function to run evaluation on several batches, and return the average metrics'''\n",
    "    \n",
    "    varexp_metrics = {name : {} for name in buffer.submodules.keys()}\n",
    "    ce_metrics = {name : {} for name in buffer.submodules.keys()}\n",
    "\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        # get varexp metrics\n",
    "        initial_acts, input_acts, output_acts, layernorm_scales = next(buffer)\n",
    "        batch_varexp_metrics = suite.evaluate_varexp_batch(\n",
    "            initial_acts,\n",
    "            input_acts, \n",
    "            output_acts,\n",
    "            layernorm_scales,\n",
    "            use_sparse_connections=use_sparse_connections\n",
    "            )\n",
    "\n",
    "        # get CE metrics\n",
    "        b = buffer.refresh_batch_size\n",
    "        buffer.refresh_batch_size = ce_batch_size\n",
    "        tokens = buffer.token_batch()\n",
    "        batch_ce_metrics = suite.evaluate_ce_batch(\n",
    "            model, \n",
    "            tokens, \n",
    "            initial_submodule,\n",
    "            submodules,\n",
    "            layernorm_submodules,\n",
    "            use_sparse_connections=use_sparse_connections\n",
    "            )\n",
    "        buffer.refresh_batch_size = b\n",
    "\n",
    "        for name in ce_metrics.keys():\n",
    "            for metric in batch_ce_metrics[name].keys():\n",
    "                ce_metrics[name][metric] = ce_metrics[name].get(metric, 0) + batch_ce_metrics[name].get(metric, 0) / n_batches\n",
    "            for metric in batch_varexp_metrics[name].keys():\n",
    "                varexp_metrics[name][metric] = varexp_metrics[name].get(metric, 0) + batch_varexp_metrics[name].get(metric, 0) / n_batches\n",
    "           \n",
    "    return varexp_metrics, ce_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 44.45 GiB of which 3.13 GiB is free. Process 2130634 has 41.31 GiB memory in use. Of the allocated memory 37.24 GiB is allocated by PyTorch, and 3.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m varexp_metrics, ce_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mrun_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mce_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_sparse_connections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36mrun_evaluation\u001b[0;34m(suite, buffer, n_batches, ce_batch_size, use_sparse_connections)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_batches)):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# get varexp metrics\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     initial_acts, input_acts, output_acts, layernorm_scales \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(buffer)\n\u001b[0;32m---> 16\u001b[0m     batch_varexp_metrics \u001b[38;5;241m=\u001b[39m \u001b[43msuite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_varexp_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_acts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_acts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayernorm_scales\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sparse_connections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_sparse_connections\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# get CE metrics\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     b \u001b[38;5;241m=\u001b[39m buffer\u001b[38;5;241m.\u001b[39mrefresh_batch_size\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary_learning/notebooks/../trainers/scae.py:329\u001b[0m, in \u001b[0;36mSCAESuite.evaluate_varexp_batch\u001b[0;34m(self, initial_acts, input_acts, target_acts, layernorm_scales, use_sparse_connections, normalize_batch)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# Get reconstructions based on forward pass type\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_sparse_connections:\n\u001b[0;32m--> 329\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapprox_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayernorm_scales\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvanilla_forward(input_acts)\n",
      "File \u001b[0;32m~/dictionary_learning/notebooks/../trainers/scae.py:259\u001b[0m, in \u001b[0;36mSCAESuite.approx_forward\u001b[0;34m(self, initial_acts, inputs, layernorm_scales)\u001b[0m\n\u001b[1;32m    256\u001b[0m     up_idxs \u001b[38;5;241m=\u001b[39m topk_info[up_name][\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# [batch, k_up]\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     up_vals \u001b[38;5;241m=\u001b[39m topk_info[up_name][\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# [batch, k_up]\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m     contributions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pruned_contributions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mup_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdown_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mup_idxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdown_idxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mup_vals\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, k_down]\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     approx_acts \u001b[38;5;241m=\u001b[39m approx_acts \u001b[38;5;241m+\u001b[39m contributions\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# divide by layernorm scale\u001b[39;00m\n",
      "File \u001b[0;32m~/dictionary_learning/notebooks/../trainers/scae.py:135\u001b[0m, in \u001b[0;36mSCAESuite.get_pruned_contributions\u001b[0;34m(self, up_name, down_name, up_indices, down_indices, up_vals)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Get the corresponding upstream values\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# [batch, k_down, C, k_up]\u001b[39;00m\n\u001b[1;32m    134\u001b[0m up_vals_expanded \u001b[38;5;241m=\u001b[39m up_vals\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, k_down, C, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 135\u001b[0m connected_vals \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mup_vals_expanded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mup_vals_expanded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Get the corresponding virtual weights\u001b[39;00m\n\u001b[1;32m    138\u001b[0m active_up_vectors \u001b[38;5;241m=\u001b[39m up_decoder[:, up_indices\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;66;03m# [batch*k_up, d_in]\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 44.45 GiB of which 3.13 GiB is free. Process 2130634 has 41.31 GiB memory in use. Of the allocated memory 37.24 GiB is allocated by PyTorch, and 3.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "varexp_metrics, ce_metrics = run_evaluation(\n",
    "    suite, \n",
    "    buffer, \n",
    "    n_batches=1, \n",
    "    ce_batch_size=1,\n",
    "    use_sparse_connections=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean loss = 2.603\n",
      "\n",
      "Module  CE increase  CE expl FVU\n",
      "mlp_0   2.052        63%     156%\n",
      "mlp_1   3.324        -9061%     501%\n",
      "mlp_2   4.008        -1937%     1613%\n",
      "mlp_3   4.290        -2960%     3161%\n",
      "mlp_4   5.054        -4547%     2836%\n",
      "mlp_5   4.334        -2453%     1951%\n",
      "mlp_6   3.739        -2183%     1089%\n",
      "mlp_7   3.900        -3212%     641%\n",
      "mlp_8   3.856        -3087%     457%\n",
      "mlp_9   4.637        -3104%     478%\n",
      "mlp_10   1.015        -373%     206%\n",
      "mlp_11   2.167        -365%     265%\n",
      "\n",
      "attn_0   0.008        100%     1%\n",
      "attn_1   -0.005        156%     3%\n",
      "attn_2   0.008        88%     5%\n",
      "attn_3   0.007        93%     6%\n",
      "attn_4   -0.005        104%     7%\n",
      "attn_5   0.003        91%     7%\n",
      "attn_6   0.004        96%     7%\n",
      "attn_7   0.001        120%     7%\n",
      "attn_8   0.007        91%     8%\n",
      "attn_9   -0.006        108%     7%\n",
      "attn_10   0.005        90%     6%\n",
      "attn_11   -0.000        100%     1%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Clean loss = {ce_metrics['mlp_0']['loss_original']:.3f}\\n\")\n",
    "\n",
    "print(\"Module  CE increase  CE expl FVU\")\n",
    "for name in [k for k in ce_metrics.keys() if 'mlp' in k]:\n",
    "    print(f\"{name}   {ce_metrics[name]['loss_reconstructed'] - ce_metrics[name]['loss_original']:.3f}        {ce_metrics[name]['frac_recovered']*100:.0f}%     {varexp_metrics[name]['FVU']*100:.0f}%\")\n",
    "\n",
    "print()\n",
    "\n",
    "for name in [k for k in ce_metrics.keys() if 'attn' in k]:\n",
    "    print(f\"{name}   {ce_metrics[name]['loss_reconstructed'] - ce_metrics[name]['loss_original']:.3f}        {ce_metrics[name]['frac_recovered']*100:.0f}%     {varexp_metrics[name]['FVU']*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
