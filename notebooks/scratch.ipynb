{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('/root/dictionary_learning')\n",
    "from buffer import SimpleBuffer\n",
    "from trainers.scae import SCAESuite\n",
    "from utils import load_model_with_folded_ln2, load_iterable_dataset\n",
    "from find_top_connections import generate_fake_connections\n",
    "from trainers.scae import SCAESuite\n",
    "import pickle\n",
    "\n",
    "import torch as t\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Jacob's token but feel free to use\n",
    "login(\"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\")\n",
    "device = \"cuda:0\" if t.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-33m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "DTYPE = t.bfloat16\n",
    "data = load_iterable_dataset('roneneldan/TinyStories')\n",
    "buffer = SimpleBuffer(data, \"tiny-stories-33m\", device=device, dtype=DTYPE, refresh_batch_size=256, ctx_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache, tokens = next(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/root/dictionary_learning/connections_TinyStories-33M_100.pkl\", \"rb\") as f:\n",
    "    connections = pickle.load(f)\n",
    "\n",
    "for layer in range(buffer.model.cfg.n_layers):\n",
    "    connections[f'attn_{layer}'] = {k: v for (k, v) in connections[f'mlp_{layer}'].items() \n",
    "                                    if int(k.split('_')[1]) < layer}\n",
    "fake_connections = generate_fake_connections(\n",
    "    connections,\n",
    "    num_features=768*4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = SCAESuite(\n",
    "    buffer.model,\n",
    "    k=128,\n",
    "    n_features=768*4,\n",
    "    connections=fake_connections,\n",
    "    dtype=DTYPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.set_grad_enabled(True)\n",
    "import gc\n",
    "gc.collect()\n",
    "t.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down_name attn_0\n",
      "down_name mlp_0\n",
      "up_name attn_0\n",
      "down_name attn_1\n",
      "up_name attn_0\n",
      "\n",
      "Starting get_pruned_contribs_attn for up_name=attn_0, down_name=attn_1\n",
      "up_facts shape: torch.Size([32, 128, 3072])\n",
      "W_O shape: torch.Size([16, 48, 768])\n",
      "W_V shape: torch.Size([16, 768, 48])\n",
      "W_OV shape: torch.Size([16, 768, 768])\n",
      "temp shape: torch.Size([16, 3072, 768])\n",
      "up_decoder shape: torch.Size([768, 3072])\n",
      "connection_mask shape: torch.Size([3072, 3072])\n",
      "i_indices shape: torch.Size([97495]), j_indices shape: torch.Size([97495])\n",
      "selected_temp shape: torch.Size([16, 97495, 768])\n",
      "selected_up shape: torch.Size([768, 97495])\n",
      "values shape: torch.Size([16, 97495])\n",
      "up_facts_selected shape: torch.Size([32, 128, 97495])\n",
      "contributions shape before scatter_add: torch.Size([32, 128, 16, 3072])\n",
      "scaled shape for head 0: torch.Size([32, 128, 97495])\n",
      "scaled shape for head 1: torch.Size([32, 128, 97495])\n",
      "scaled shape for head 2: torch.Size([32, 128, 97495])\n",
      "scaled shape for head 3: torch.Size([32, 128, 97495])\n",
      "scaled shape for head 4: torch.Size([32, 128, 97495])\n",
      "scaled shape for head 5: torch.Size([32, 128, 97495])\n",
      "scaled shape for head 6: torch.Size([32, 128, 97495])\n",
      "scaled shape for head 7: torch.Size([32, 128, 97495])\n",
      "scaled shape for head 8: torch.Size([32, 128, 97495])\n",
      "scaled shape for head 9: torch.Size([32, 128, 97495])\n",
      "scaled shape for head 10: torch.Size([32, 128, 97495])\n",
      "scaled shape for head 11: torch.Size([32, 128, 97495])\n",
      "scaled shape for head 12: torch.Size([32, 128, 97495])\n",
      "scaled shape for head 13: torch.Size([32, 128, 97495])\n",
      "scaled shape for head 14: torch.Size([32, 128, 97495])\n",
      "scaled shape for head 15: torch.Size([32, 128, 97495])\n",
      "contributions shape after scatter_add: torch.Size([32, 128, 16, 3072])\n",
      "probs shape: torch.Size([32, 16, 128, 128])\n",
      "final_contribs shape: torch.Size([32, 128, 3072])\n",
      "up_name mlp_0\n",
      "\n",
      "Starting get_pruned_contribs_attn for up_name=mlp_0, down_name=attn_1\n",
      "up_facts shape: torch.Size([32, 128, 3072])\n",
      "W_O shape: torch.Size([16, 48, 768])\n",
      "W_V shape: torch.Size([16, 768, 48])\n",
      "W_OV shape: torch.Size([16, 768, 768])\n",
      "temp shape: torch.Size([16, 3072, 768])\n",
      "up_decoder shape: torch.Size([768, 3072])\n",
      "connection_mask shape: torch.Size([3072, 3072])\n",
      "i_indices shape: torch.Size([87312]), j_indices shape: torch.Size([87312])\n",
      "selected_temp shape: torch.Size([16, 87312, 768])\n",
      "selected_up shape: torch.Size([768, 87312])\n",
      "values shape: torch.Size([16, 87312])\n",
      "up_facts_selected shape: torch.Size([32, 128, 87312])\n",
      "contributions shape before scatter_add: torch.Size([32, 128, 16, 3072])\n",
      "scaled shape for head 0: torch.Size([32, 128, 87312])\n",
      "scaled shape for head 1: torch.Size([32, 128, 87312])\n",
      "scaled shape for head 2: torch.Size([32, 128, 87312])\n",
      "scaled shape for head 3: torch.Size([32, 128, 87312])\n",
      "scaled shape for head 4: torch.Size([32, 128, 87312])\n",
      "scaled shape for head 5: torch.Size([32, 128, 87312])\n",
      "scaled shape for head 6: torch.Size([32, 128, 87312])\n",
      "scaled shape for head 7: torch.Size([32, 128, 87312])\n",
      "scaled shape for head 8: torch.Size([32, 128, 87312])\n",
      "scaled shape for head 9: torch.Size([32, 128, 87312])\n",
      "scaled shape for head 10: torch.Size([32, 128, 87312])\n",
      "scaled shape for head 11: torch.Size([32, 128, 87312])\n",
      "scaled shape for head 12: torch.Size([32, 128, 87312])\n",
      "scaled shape for head 13: torch.Size([32, 128, 87312])\n",
      "scaled shape for head 14: torch.Size([32, 128, 87312])\n",
      "scaled shape for head 15: torch.Size([32, 128, 87312])\n",
      "contributions shape after scatter_add: torch.Size([32, 128, 16, 3072])\n",
      "probs shape: torch.Size([32, 16, 128, 128])\n",
      "final_contribs shape: torch.Size([32, 128, 3072])\n",
      "down_name mlp_1\n",
      "up_name attn_0\n",
      "up_name mlp_0\n",
      "up_name attn_1\n",
      "down_name attn_2\n",
      "up_name attn_0\n",
      "\n",
      "Starting get_pruned_contribs_attn for up_name=attn_0, down_name=attn_2\n",
      "up_facts shape: torch.Size([32, 128, 3072])\n",
      "W_O shape: torch.Size([16, 48, 768])\n",
      "W_V shape: torch.Size([16, 768, 48])\n",
      "W_OV shape: torch.Size([16, 768, 768])\n",
      "temp shape: torch.Size([16, 3072, 768])\n",
      "up_decoder shape: torch.Size([768, 3072])\n",
      "connection_mask shape: torch.Size([3072, 3072])\n",
      "i_indices shape: torch.Size([43054]), j_indices shape: torch.Size([43054])\n",
      "selected_temp shape: torch.Size([16, 43054, 768])\n",
      "selected_up shape: torch.Size([768, 43054])\n",
      "values shape: torch.Size([16, 43054])\n",
      "up_facts_selected shape: torch.Size([32, 128, 43054])\n",
      "contributions shape before scatter_add: torch.Size([32, 128, 16, 3072])\n",
      "scaled shape for head 0: torch.Size([32, 128, 43054])\n",
      "scaled shape for head 1: torch.Size([32, 128, 43054])\n",
      "scaled shape for head 2: torch.Size([32, 128, 43054])\n",
      "scaled shape for head 3: torch.Size([32, 128, 43054])\n",
      "scaled shape for head 4: torch.Size([32, 128, 43054])\n",
      "scaled shape for head 5: torch.Size([32, 128, 43054])\n",
      "scaled shape for head 6: torch.Size([32, 128, 43054])\n",
      "scaled shape for head 7: torch.Size([32, 128, 43054])\n",
      "scaled shape for head 8: torch.Size([32, 128, 43054])\n",
      "scaled shape for head 9: torch.Size([32, 128, 43054])\n",
      "scaled shape for head 10: torch.Size([32, 128, 43054])\n",
      "scaled shape for head 11: torch.Size([32, 128, 43054])\n",
      "scaled shape for head 12: torch.Size([32, 128, 43054])\n",
      "scaled shape for head 13: torch.Size([32, 128, 43054])\n",
      "scaled shape for head 14: torch.Size([32, 128, 43054])\n",
      "scaled shape for head 15: torch.Size([32, 128, 43054])\n",
      "contributions shape after scatter_add: torch.Size([32, 128, 16, 3072])\n",
      "probs shape: torch.Size([32, 16, 128, 128])\n",
      "final_contribs shape: torch.Size([32, 128, 3072])\n",
      "up_name mlp_0\n",
      "\n",
      "Starting get_pruned_contribs_attn for up_name=mlp_0, down_name=attn_2\n",
      "up_facts shape: torch.Size([32, 128, 3072])\n",
      "W_O shape: torch.Size([16, 48, 768])\n",
      "W_V shape: torch.Size([16, 768, 48])\n",
      "W_OV shape: torch.Size([16, 768, 768])\n",
      "temp shape: torch.Size([16, 3072, 768])\n",
      "up_decoder shape: torch.Size([768, 3072])\n",
      "connection_mask shape: torch.Size([3072, 3072])\n",
      "i_indices shape: torch.Size([31959]), j_indices shape: torch.Size([31959])\n",
      "selected_temp shape: torch.Size([16, 31959, 768])\n",
      "selected_up shape: torch.Size([768, 31959])\n",
      "values shape: torch.Size([16, 31959])\n",
      "up_facts_selected shape: torch.Size([32, 128, 31959])\n",
      "contributions shape before scatter_add: torch.Size([32, 128, 16, 3072])\n",
      "scaled shape for head 0: torch.Size([32, 128, 31959])\n",
      "scaled shape for head 1: torch.Size([32, 128, 31959])\n",
      "scaled shape for head 2: torch.Size([32, 128, 31959])\n",
      "scaled shape for head 3: torch.Size([32, 128, 31959])\n",
      "scaled shape for head 4: torch.Size([32, 128, 31959])\n",
      "scaled shape for head 5: torch.Size([32, 128, 31959])\n",
      "scaled shape for head 6: torch.Size([32, 128, 31959])\n",
      "scaled shape for head 7: torch.Size([32, 128, 31959])\n",
      "scaled shape for head 8: torch.Size([32, 128, 31959])\n",
      "scaled shape for head 9: torch.Size([32, 128, 31959])\n",
      "scaled shape for head 10: torch.Size([32, 128, 31959])\n",
      "scaled shape for head 11: torch.Size([32, 128, 31959])\n",
      "scaled shape for head 12: torch.Size([32, 128, 31959])\n",
      "scaled shape for head 13: torch.Size([32, 128, 31959])\n",
      "scaled shape for head 14: torch.Size([32, 128, 31959])\n",
      "scaled shape for head 15: torch.Size([32, 128, 31959])\n",
      "contributions shape after scatter_add: torch.Size([32, 128, 16, 3072])\n",
      "probs shape: torch.Size([32, 16, 128, 128])\n",
      "final_contribs shape: torch.Size([32, 128, 3072])\n",
      "up_name attn_1\n",
      "\n",
      "Starting get_pruned_contribs_attn for up_name=attn_1, down_name=attn_2\n",
      "up_facts shape: torch.Size([32, 128, 3072])\n",
      "W_O shape: torch.Size([16, 48, 768])\n",
      "W_V shape: torch.Size([16, 768, 48])\n",
      "W_OV shape: torch.Size([16, 768, 768])\n",
      "temp shape: torch.Size([16, 3072, 768])\n",
      "up_decoder shape: torch.Size([768, 3072])\n",
      "connection_mask shape: torch.Size([3072, 3072])\n",
      "i_indices shape: torch.Size([38035]), j_indices shape: torch.Size([38035])\n",
      "selected_temp shape: torch.Size([16, 38035, 768])\n",
      "selected_up shape: torch.Size([768, 38035])\n",
      "values shape: torch.Size([16, 38035])\n",
      "up_facts_selected shape: torch.Size([32, 128, 38035])\n",
      "contributions shape before scatter_add: torch.Size([32, 128, 16, 3072])\n",
      "scaled shape for head 0: torch.Size([32, 128, 38035])\n",
      "scaled shape for head 1: torch.Size([32, 128, 38035])\n",
      "scaled shape for head 2: torch.Size([32, 128, 38035])\n",
      "scaled shape for head 3: torch.Size([32, 128, 38035])\n",
      "scaled shape for head 4: torch.Size([32, 128, 38035])\n",
      "scaled shape for head 5: torch.Size([32, 128, 38035])\n",
      "scaled shape for head 6: torch.Size([32, 128, 38035])\n",
      "scaled shape for head 7: torch.Size([32, 128, 38035])\n",
      "scaled shape for head 8: torch.Size([32, 128, 38035])\n",
      "scaled shape for head 9: torch.Size([32, 128, 38035])\n",
      "scaled shape for head 10: torch.Size([32, 128, 38035])\n",
      "scaled shape for head 11: torch.Size([32, 128, 38035])\n",
      "scaled shape for head 12: torch.Size([32, 128, 38035])\n",
      "scaled shape for head 13: torch.Size([32, 128, 38035])\n",
      "scaled shape for head 14: torch.Size([32, 128, 38035])\n",
      "scaled shape for head 15: torch.Size([32, 128, 38035])\n",
      "contributions shape after scatter_add: torch.Size([32, 128, 16, 3072])\n",
      "probs shape: torch.Size([32, 16, 128, 128])\n",
      "final_contribs shape: torch.Size([32, 128, 3072])\n",
      "up_name mlp_1\n",
      "\n",
      "Starting get_pruned_contribs_attn for up_name=mlp_1, down_name=attn_2\n",
      "up_facts shape: torch.Size([32, 128, 3072])\n",
      "W_O shape: torch.Size([16, 48, 768])\n",
      "W_V shape: torch.Size([16, 768, 48])\n",
      "W_OV shape: torch.Size([16, 768, 768])\n",
      "temp shape: torch.Size([16, 3072, 768])\n",
      "up_decoder shape: torch.Size([768, 3072])\n",
      "connection_mask shape: torch.Size([3072, 3072])\n",
      "i_indices shape: torch.Size([36474]), j_indices shape: torch.Size([36474])\n",
      "selected_temp shape: torch.Size([16, 36474, 768])\n",
      "selected_up shape: torch.Size([768, 36474])\n",
      "values shape: torch.Size([16, 36474])\n",
      "up_facts_selected shape: torch.Size([32, 128, 36474])\n",
      "contributions shape before scatter_add: torch.Size([32, 128, 16, 3072])\n",
      "scaled shape for head 0: torch.Size([32, 128, 36474])\n",
      "scaled shape for head 1: torch.Size([32, 128, 36474])\n",
      "scaled shape for head 2: torch.Size([32, 128, 36474])\n",
      "scaled shape for head 3: torch.Size([32, 128, 36474])\n",
      "scaled shape for head 4: torch.Size([32, 128, 36474])\n",
      "scaled shape for head 5: torch.Size([32, 128, 36474])\n",
      "scaled shape for head 6: torch.Size([32, 128, 36474])\n",
      "scaled shape for head 7: torch.Size([32, 128, 36474])\n",
      "scaled shape for head 8: torch.Size([32, 128, 36474])\n",
      "scaled shape for head 9: torch.Size([32, 128, 36474])\n",
      "scaled shape for head 10: torch.Size([32, 128, 36474])\n",
      "scaled shape for head 11: torch.Size([32, 128, 36474])\n",
      "scaled shape for head 12: torch.Size([32, 128, 36474])\n",
      "scaled shape for head 13: torch.Size([32, 128, 36474])\n",
      "scaled shape for head 14: torch.Size([32, 128, 36474])\n",
      "scaled shape for head 15: torch.Size([32, 128, 36474])\n",
      "contributions shape after scatter_add: torch.Size([32, 128, 16, 3072])\n",
      "probs shape: torch.Size([32, 16, 128, 128])\n",
      "final_contribs shape: torch.Size([32, 128, 3072])\n",
      "down_name mlp_2\n",
      "up_name attn_0\n",
      "up_name mlp_0\n",
      "up_name attn_1\n",
      "up_name mlp_1\n",
      "up_name attn_2\n",
      "down_name attn_3\n",
      "up_name attn_0\n",
      "\n",
      "Starting get_pruned_contribs_attn for up_name=attn_0, down_name=attn_3\n",
      "up_facts shape: torch.Size([32, 128, 3072])\n",
      "W_O shape: torch.Size([16, 48, 768])\n",
      "W_V shape: torch.Size([16, 768, 48])\n",
      "W_OV shape: torch.Size([16, 768, 768])\n",
      "temp shape: torch.Size([16, 3072, 768])\n",
      "up_decoder shape: torch.Size([768, 3072])\n",
      "connection_mask shape: torch.Size([3072, 3072])\n",
      "i_indices shape: torch.Size([22839]), j_indices shape: torch.Size([22839])\n",
      "selected_temp shape: torch.Size([16, 22839, 768])\n",
      "selected_up shape: torch.Size([768, 22839])\n",
      "values shape: torch.Size([16, 22839])\n",
      "up_facts_selected shape: torch.Size([32, 128, 22839])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 44.45 GiB of which 314.62 MiB is free. Process 2835160 has 44.13 GiB memory in use. Of the allocated memory 43.56 GiB is allocated by PyTorch, and 262.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m      4\u001b[0m t\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m----> 6\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43msuite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_pruned\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary_learning/notebooks/../trainers/scae.py:336\u001b[0m, in \u001b[0;36mSCAESuite.forward_pruned\u001b[0;34m(self, cache)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# Get contributions based on module type\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m down_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 336\u001b[0m     contributions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pruned_contribs_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mup_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdown_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mup_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# mlp\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     contributions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_pruned_contribs_mlp(cache, up_name, down_name, up_feats)\n",
      "File \u001b[0;32m~/dictionary_learning/notebooks/../trainers/scae.py:261\u001b[0m, in \u001b[0;36mSCAESuite.get_pruned_contribs_attn\u001b[0;34m(self, cache, up_name, down_name, up_facts)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mup_facts_selected shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mup_facts_selected\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    260\u001b[0m n_heads, m \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 261\u001b[0m contributions \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mup_facts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mup_facts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mup_facts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mup_facts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontributions shape before scatter_add: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontributions\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_heads):\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 44.45 GiB of which 314.62 MiB is free. Process 2835160 has 44.13 GiB memory in use. Of the allocated memory 43.56 GiB is allocated by PyTorch, and 262.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "t.set_grad_enabled(True)\n",
    "import gc\n",
    "gc.collect()\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "out = suite.forward_pruned({k: v[:16] for k, v in cache.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1843.2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*150000*768 / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
