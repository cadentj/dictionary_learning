{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch as t\n",
    "import einops\n",
    "from typing import Dict, List\n",
    "import torch.sparse as sparse\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torch as t\n",
    "import torch.sparse as sparse\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from typing import Dict, List\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from trainers.scae import SCAESuite\n",
    "from buffer import AllActivationBuffer\n",
    "from utils import load_model_with_folded_ln2, load_iterable_dataset\n",
    "from find_top_connections import get_importance_scores\n",
    "from trainers.top_k import AutoEncoderTopK\n",
    "from trainers.scae import SCAESuite, TrainerSCAESuite, TrainerConfig\n",
    "\n",
    "DTYPE = t.float32\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "t.manual_seed(42)\n",
    "t.set_grad_enabled(True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/dictionary_learning/notebooks/../trainers/scae.py:685: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = t.load(checkpoint_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "model = load_model_with_folded_ln2(\"gpt2\", device=device, torch_dtype=DTYPE)\n",
    "data = load_iterable_dataset('Skylion007/openwebtext')\n",
    "suite = SCAESuite.from_pretrained(\n",
    "    'jacobcd52/gpt2_suite_folded_ln',\n",
    "    device=device,\n",
    "    dtype=DTYPE,\n",
    "    )\n",
    "\n",
    "initial_submodule = model.transformer.h[0]\n",
    "layernorm_submodules = {}\n",
    "submodules = {}\n",
    "for layer in range(model.config.n_layer):\n",
    "    submodules[f\"mlp_{layer}\"] = (model.transformer.h[layer].mlp, \"in_and_out\")\n",
    "    submodules[f\"attn_{layer}\"] = (model.transformer.h[layer].attn, \"out\")\n",
    "\n",
    "    layernorm_submodules[f\"mlp_{layer}\"] = model.transformer.h[layer].ln_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "t.cuda.empty_cache()\n",
    "buffer = AllActivationBuffer(\n",
    "    data=data,\n",
    "    model=model,\n",
    "    submodules=submodules,\n",
    "    initial_submodule=initial_submodule,\n",
    "    layernorm_submodules=layernorm_submodules,\n",
    "    d_submodule=model.config.n_embd,\n",
    "    n_ctxs=128,\n",
    "    out_batch_size = 4,\n",
    "    refresh_batch_size = 128,\n",
    "    device=device,\n",
    "    dtype=DTYPE,\n",
    "    remove_bos=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/root/dictionary_learning/connections_100.pkl\", \"rb\") as f:\n",
    "    connections = pickle.load(f)\n",
    "\n",
    "num_features = connections['mlp_0']['attn_0'].shape[0]\n",
    "\n",
    "for k in connections['mlp_1'].keys():\n",
    "    connections['mlp_1'][k] = t.arange(0, num_features).unsqueeze(0).repeat(num_features, 1).cuda()\n",
    "\n",
    "suite.connections = connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp_0    0.042 --> 0.050\n",
      "mlp_1    0.008 --> 0.144\n",
      "mlp_2    0.001 --> 0.237\n",
      "mlp_3    0.055 --> 0.142\n",
      "mlp_4    0.097 --> 0.182\n",
      "mlp_5    0.143 --> 0.180\n",
      "mlp_6    0.174 --> 0.188\n",
      "mlp_7    0.177 --> 0.179\n",
      "mlp_8    0.171 --> 0.164\n",
      "mlp_9    0.149 --> 0.153\n",
      "mlp_10    0.111 --> 0.120\n",
      "mlp_11    0.055 --> 0.060\n"
     ]
    }
   ],
   "source": [
    "# for layer in range(12):\n",
    "#     buffer = AllActivationBuffer(\n",
    "#         data=data,\n",
    "#         model=model,\n",
    "#         submodules=submodules,\n",
    "#         initial_submodule=initial_submodule,\n",
    "#         layernorm_submodules=layernorm_submodules,\n",
    "#         d_submodule=model.config.n_embd,\n",
    "#         n_ctxs=128,\n",
    "#         out_batch_size = 512,\n",
    "#         refresh_batch_size = 128,\n",
    "#         device=device,\n",
    "#         dtype=DTYPE,\n",
    "#         remove_bos=False\n",
    "#     )       \n",
    "#     initial, input, output, ln = next(buffer)\n",
    "#     down_name = f\"mlp_{layer}\"\n",
    "#     recons = suite.aes[down_name](input[down_name])\n",
    "#     fvu_bos = (recons - output[down_name]).var(dim=0).sum() / output[down_name].var(dim=0).sum()\n",
    "\n",
    "#     buffer = AllActivationBuffer(\n",
    "#         data=data,\n",
    "#         model=model,\n",
    "#         submodules=submodules,\n",
    "#         initial_submodule=initial_submodule,\n",
    "#         layernorm_submodules=layernorm_submodules,\n",
    "#         d_submodule=model.config.n_embd,\n",
    "#         n_ctxs=128,\n",
    "#         out_batch_size = 512,\n",
    "#         refresh_batch_size = 128,\n",
    "#         device=device,\n",
    "#         dtype=DTYPE,\n",
    "#         remove_bos=True\n",
    "#     )   \n",
    "#     initial, input, output, ln = next(buffer)\n",
    "#     recons = suite.aes[down_name](input[down_name])\n",
    "#     fvu_no_bos = (recons - output[down_name]).var(dim=0).sum() / output[down_name].var(dim=0).sum()\n",
    "\n",
    "#     print(f\"mlp_{layer}    {fvu_bos.item():.3f} --> {fvu_no_bos.item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(770.5536, device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial, input, output, ln = next(buffer)\n",
    "output['mlp_8'].var(0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(367.2291, device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input['mlp_8'].var(0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2462.8831, device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['mlp_1'].var(0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resid_mid_1 = initial + output['attn_0'] + output['mlp_0'] + output['attn_1'] \n",
    "# mlp_in_1 = resid_mid_1 / ln['mlp_1'].unsqueeze(1)\n",
    "# recons = suite.aes['mlp_1'](mlp_in_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1726, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "down_name = 'mlp_8'\n",
    "recons = suite.aes[down_name](input[down_name])\n",
    "(recons - output[down_name]).var(dim=0).sum() / output[down_name].var(dim=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0079, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(recons - output[down_name]).pow(2).sum(-1).mean(0) / output[down_name].var(dim=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = t.randn(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2821, 2.8148, 1.0993, 3.0074, 0.2641, 0.7284, 0.3715, 1.3562, 0.7572,\n",
       "        0.2281])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x - x.mean(1, keepdim=True)).pow(2).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2821, 2.8148, 1.0993, 3.0074, 0.2641, 0.7284, 0.3715, 1.3562, 0.7572,\n",
       "        0.2281])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.var(dim=1, correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = suite.pruned_forward_train(initial, input, ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_recons = out['mlp_1']['pruned_reconstruction']\n",
    "pruned_recons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0330, device='cuda:0', dtype=torch.bfloat16, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pruned_recons - recons).pow(2).mean() / recons.pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2637, device='cuda:0', dtype=torch.bfloat16, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(recons - output['mlp_1']).pow(2).mean() / output['mlp_1'].pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_cfg = TrainerConfig(\n",
    "    steps=1000,\n",
    "    lr_decay_start_proportion=0.8,\n",
    "    dead_feature_threshold=10_000_000,\n",
    "    base_lr=2e-4,\n",
    "    n_threshold=100,\n",
    "    n_random=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TrainerSCAESuite(\n",
    "    suite,\n",
    "    config=trainer_cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27392.0\n",
      "34560.0\n",
      "26496.0\n",
      "39936.0\n",
      "35840.0\n",
      "29952.0\n",
      "27392.0\n",
      "34560.0\n",
      "27520.0\n",
      "27904.0\n",
      "28800.0\n",
      "38400.0\n",
      "30848.0\n",
      "28288.0\n",
      "25344.0\n",
      "24192.0\n",
      "26624.0\n",
      "34816.0\n",
      "37376.0\n",
      "47104.0\n",
      "36096.0\n",
      "33024.0\n",
      "57856.0\n",
      "27008.0\n",
      "28288.0\n",
      "28800.0\n",
      "36608.0\n",
      "26368.0\n",
      "29440.0\n",
      "34560.0\n",
      "26880.0\n",
      "27264.0\n",
      "27136.0\n",
      "25216.0\n",
      "28672.0\n",
      "28544.0\n",
      "33792.0\n",
      "27392.0\n",
      "27264.0\n",
      "27136.0\n",
      "36608.0\n",
      "26240.0\n",
      "28416.0\n",
      "26880.0\n",
      "26496.0\n",
      "25984.0\n",
      "32384.0\n",
      "26112.0\n",
      "26624.0\n",
      "27264.0\n",
      "26880.0\n",
      "26368.0\n",
      "26112.0\n",
      "29952.0\n",
      "26624.0\n",
      "25728.0\n",
      "26368.0\n",
      "33024.0\n",
      "25216.0\n",
      "40704.0\n",
      "25344.0\n",
      "25600.0\n",
      "25856.0\n",
      "26368.0\n",
      "25216.0\n",
      "25472.0\n",
      "34048.0\n",
      "24960.0\n",
      "26624.0\n",
      "26112.0\n",
      "28928.0\n",
      "23808.0\n",
      "27520.0\n",
      "25856.0\n",
      "23168.0\n",
      "26624.0\n",
      "27776.0\n",
      "23040.0\n",
      "23936.0\n",
      "25216.0\n",
      "26240.0\n",
      "23040.0\n",
      "25344.0\n",
      "26880.0\n",
      "25472.0\n",
      "25600.0\n",
      "33024.0\n",
      "27648.0\n",
      "27648.0\n",
      "25344.0\n",
      "23936.0\n",
      "25088.0\n",
      "41984.0\n",
      "37888.0\n",
      "35072.0\n",
      "31104.0\n",
      "30336.0\n",
      "27136.0\n",
      "33792.0\n",
      "28672.0\n",
      "24704.0\n",
      "23168.0\n",
      "24064.0\n",
      "26112.0\n",
      "31232.0\n",
      "26368.0\n",
      "23168.0\n",
      "23552.0\n",
      "33024.0\n",
      "23168.0\n",
      "23552.0\n",
      "25088.0\n",
      "24960.0\n",
      "25984.0\n",
      "24576.0\n",
      "24960.0\n",
      "32000.0\n",
      "25856.0\n",
      "23680.0\n",
      "22400.0\n",
      "30720.0\n",
      "28416.0\n",
      "25856.0\n",
      "24320.0\n",
      "31232.0\n",
      "30208.0\n",
      "27392.0\n",
      "26880.0\n",
      "24448.0\n",
      "27392.0\n",
      "30208.0\n",
      "26112.0\n",
      "32768.0\n",
      "34560.0\n",
      "26496.0\n",
      "24448.0\n",
      "25088.0\n",
      "25472.0\n",
      "26112.0\n",
      "23552.0\n",
      "26624.0\n",
      "26496.0\n",
      "25088.0\n",
      "32768.0\n",
      "32768.0\n",
      "25216.0\n",
      "25600.0\n",
      "25088.0\n",
      "33792.0\n",
      "25088.0\n",
      "22784.0\n",
      "24320.0\n",
      "25088.0\n",
      "24064.0\n",
      "24448.0\n",
      "23680.0\n",
      "31360.0\n",
      "24576.0\n",
      "30080.0\n",
      "23680.0\n",
      "23296.0\n",
      "24960.0\n",
      "27520.0\n",
      "31360.0\n",
      "23552.0\n",
      "23552.0\n",
      "23680.0\n",
      "24320.0\n",
      "27520.0\n",
      "33024.0\n",
      "22400.0\n",
      "24192.0\n",
      "25856.0\n",
      "24448.0\n",
      "22912.0\n",
      "26240.0\n",
      "23040.0\n",
      "24320.0\n",
      "24704.0\n",
      "25856.0\n",
      "29440.0\n",
      "28032.0\n",
      "33280.0\n",
      "31616.0\n",
      "33024.0\n",
      "25216.0\n",
      "26240.0\n",
      "33280.0\n",
      "28800.0\n",
      "24832.0\n",
      "23680.0\n",
      "25088.0\n",
      "32000.0\n",
      "25728.0\n",
      "24832.0\n",
      "23808.0\n",
      "34816.0\n",
      "25600.0\n",
      "24960.0\n",
      "23936.0\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "buffer.out_batch_size = 32\n",
    "for _ in range(200):\n",
    "    initial_acts, input_acts, target_acts, ln_scales = next(buffer)\n",
    "    loss = trainer.update(\n",
    "        500,\n",
    "        initial_acts,\n",
    "        input_acts,\n",
    "        target_acts,\n",
    "        ln_scales)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 768])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer.out_batch_size = 8\n",
    "initial_acts, input_acts, target_acts, ln_scales = next(buffer)\n",
    "initial_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out = trainer.suite.pruned_forward_train(\n",
    "    initial_acts,\n",
    "    input_acts,\n",
    "    ln_scales,\n",
    "    n_random=0,\n",
    "    n_threshold=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "t.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "with t.no_grad():\n",
    "    test_out = suite.pruned_forward_test(\n",
    "    initial_acts,\n",
    "    input_acts,\n",
    "    ln_scales,\n",
    "    return_topk=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 FVU: 0.0\n",
      "Layer 1 FVU: 0.0\n",
      "Layer 2 FVU: 0.0\n",
      "Layer 3 FVU: 0.0\n",
      "Layer 4 FVU: 0.0\n",
      "Layer 5 FVU: 0.0\n",
      "Layer 6 FVU: 0.0\n",
      "Layer 7 FVU: 0.0\n",
      "Layer 8 FVU: 0.0\n",
      "Layer 9 FVU: 0.0\n",
      "Layer 10 FVU: 0.0\n",
      "Layer 11 FVU: 0.0\n",
      "Layer 0 FVU: 0.361328125\n",
      "Layer 1 FVU: 2.71875\n",
      "Layer 2 FVU: 1.2578125\n",
      "Layer 3 FVU: 3.0\n",
      "Layer 4 FVU: 3.265625\n",
      "Layer 5 FVU: 1.5\n",
      "Layer 6 FVU: 1.6484375\n",
      "Layer 7 FVU: 1.078125\n",
      "Layer 8 FVU: 0.6796875\n",
      "Layer 9 FVU: 0.56640625\n",
      "Layer 10 FVU: 0.4375\n",
      "Layer 11 FVU: 1.1875\n"
     ]
    }
   ],
   "source": [
    "for module in [\"attn\", \"mlp\"]:\n",
    "    for layer in range(model.config.n_layer):\n",
    "        \n",
    "        test = test_out[0][f'{module}_{layer}']\n",
    "        train = train_out[f'{module}_{layer}']['pruned_reconstruction']\n",
    "        fvu = (test-train).pow(2).mean() / test.pow(2).mean()\n",
    "        print(f\"Layer {layer} FVU: {fvu.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 FVU: 0.0\n",
      "Layer 1 FVU: 0.0\n",
      "Layer 2 FVU: 0.0\n",
      "Layer 3 FVU: 0.0\n",
      "Layer 4 FVU: 0.0\n",
      "Layer 5 FVU: 0.0\n",
      "Layer 6 FVU: 0.0\n",
      "Layer 7 FVU: 0.0\n",
      "Layer 8 FVU: 0.0\n",
      "Layer 9 FVU: 0.0\n",
      "Layer 10 FVU: 0.0\n",
      "Layer 11 FVU: 0.0\n",
      "Layer 0 FVU: 0.2392578125\n",
      "Layer 1 FVU: 2.78125\n",
      "Layer 2 FVU: 1.0390625\n",
      "Layer 3 FVU: 1.6953125\n",
      "Layer 4 FVU: 2.5625\n",
      "Layer 5 FVU: 1.65625\n",
      "Layer 6 FVU: 1.75\n",
      "Layer 7 FVU: 1.3515625\n",
      "Layer 8 FVU: 0.8515625\n",
      "Layer 9 FVU: 0.609375\n",
      "Layer 10 FVU: 0.50390625\n",
      "Layer 11 FVU: 2.640625\n"
     ]
    }
   ],
   "source": [
    "for module in [\"attn\", \"mlp\"]:\n",
    "    for layer in range(model.config.n_layer):\n",
    "        \n",
    "        test = test_out[0][f'{module}_{layer}']\n",
    "        train = train_out[f'{module}_{layer}']['pruned_reconstruction']\n",
    "        fvu = (test-train).pow(2).mean() / test.pow(2).mean()\n",
    "        print(f\"Layer {layer} FVU: {fvu.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 FVU: 0.2734375\n",
      "Layer 1 FVU: 0.365234375\n",
      "Layer 2 FVU: 0.486328125\n",
      "Layer 3 FVU: 0.8046875\n",
      "Layer 4 FVU: 0.9296875\n",
      "Layer 5 FVU: 0.482421875\n",
      "Layer 6 FVU: 0.2890625\n",
      "Layer 7 FVU: 0.41015625\n",
      "Layer 8 FVU: 0.81640625\n",
      "Layer 9 FVU: 0.796875\n",
      "Layer 10 FVU: 0.7109375\n",
      "Layer 11 FVU: 0.78125\n"
     ]
    }
   ],
   "source": [
    "for module in [\"mlp\"]:\n",
    "    for layer in range(model.config.n_layer):\n",
    "        test = test_out[1][f'{module}_{layer}']['values']\n",
    "        train = train_out[f'{module}_{layer}']['topk']\n",
    "        fvu = (test-train).pow(2).mean() / test.pow(2).mean()\n",
    "        print(f\"Layer {layer} FVU: {fvu.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 FVU: 0.1416015625\n",
      "Layer 1 FVU: 0.2890625\n",
      "Layer 2 FVU: 0.515625\n",
      "Layer 3 FVU: 0.69921875\n",
      "Layer 4 FVU: 0.83984375\n",
      "Layer 5 FVU: 0.53125\n",
      "Layer 6 FVU: 0.310546875\n",
      "Layer 7 FVU: 0.462890625\n",
      "Layer 8 FVU: 0.8515625\n",
      "Layer 9 FVU: 1.0546875\n",
      "Layer 10 FVU: 0.6796875\n",
      "Layer 11 FVU: 0.63671875\n"
     ]
    }
   ],
   "source": [
    "for module in [\"mlp\"]:\n",
    "    for layer in range(model.config.n_layer):\n",
    "        test = test_out[1][f'{module}_{layer}']['values']\n",
    "        train = train_out[f'{module}_{layer}']['topk']\n",
    "        fvu = (test-train).pow(2).mean() / test.pow(2).mean()\n",
    "        print(f\"Layer {layer} FVU: {fvu.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
